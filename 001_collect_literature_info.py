#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
001 - Collect Literature Information
æ”¶é›†Zoteroæ–‡çŒ®ä¿¡æ¯å¹¶ä¿å­˜åˆ°Excelæ–‡ä»¶
"""

import os
import sys
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path
from tqdm import tqdm

# å¯¼å…¥å·²æœ‰çš„æ¨¡å—
from main import ZoteroManager

class LiteratureCollector:
    """æ–‡çŒ®ä¿¡æ¯æ”¶é›†å™¨"""
    
    def __init__(self, user_id: str = None, api_key: str = None):
        """åˆå§‹åŒ–æ”¶é›†å™¨"""
        self.zotero = ZoteroManager(user_id, api_key)
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
    
    def get_all_items(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ–‡çŒ®é¡¹ç›®"""
        all_items = []
        start = 0
        limit = 100
        
        print("ğŸ“š æ­£åœ¨è·å–æ‰€æœ‰æ–‡çŒ®...")
        
        while True:
            try:
                items = self.zotero.get_items(limit=limit, start=start)
                if not items:
                    break
                    
                all_items.extend(items)
                start += limit
                print(f"   å·²è·å– {len(all_items)} ç¯‡æ–‡çŒ®...")
                
            except Exception as e:
                print(f"è·å–æ–‡çŒ®å¤±è´¥: {e}")
                break
        
        print(f"âœ… æ€»å…±è·å–åˆ° {len(all_items)} ç¯‡æ–‡çŒ®")
        return all_items
    
    def extract_literature_info(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
        data = item.get('data', {})
        
        # åŸºæœ¬ä¿¡æ¯
        item_key = data.get('key', '')
        title = data.get('title', '').strip()
        item_type = data.get('itemType', 'unknown')
        abstract = data.get('abstractNote', '').strip()
        url = data.get('url', '').strip()
        
        # ä½œè€…ä¿¡æ¯
        creators = data.get('creators', [])
        authors = []
        for creator in creators:
            if 'name' in creator:
                authors.append(creator['name'])
            elif 'firstName' in creator or 'lastName' in creator:
                first = creator.get('firstName', '')
                last = creator.get('lastName', '')
                if first and last:
                    authors.append(f"{first} {last}")
                elif last:
                    authors.append(last)
        
        authors_str = '; '.join(authors) if authors else ''
        
        # å‡ºç‰ˆä¿¡æ¯
        publication_title = data.get('publicationTitle', '').strip()
        conference_name = data.get('conferenceName', '').strip()
        journal_abbreviation = data.get('journalAbbreviation', '').strip()
        publisher = data.get('publisher', '').strip()
        
        # æ—¶é—´ä¿¡æ¯
        date = data.get('date', '').strip()
        
        # DOIå’Œæ ‡è¯†ä¿¡æ¯
        doi = data.get('DOI', '').strip()
        isbn = data.get('ISBN', '').strip()
        issn = data.get('ISSN', '').strip()
        
        # æ ‡ç­¾
        tags = [tag.get('tag', '') for tag in data.get('tags', []) if tag.get('tag')]
        tags_str = '; '.join(tags) if tags else ''
        
        # å½“å‰åˆ†ç±»ä¿¡æ¯
        collections = data.get('collections', [])
        
        # ç¡®ä¿titleå­˜åœ¨
        if not title:
            title = 'æ— æ ‡é¢˜'
        
        # æ•°æ®æ·»åŠ æ—¶é—´
        date_added = data.get('dateAdded', '')
        date_modified = data.get('dateModified', '')
        
        # å…¶ä»–æœ‰ç”¨ä¿¡æ¯
        volume = data.get('volume', '').strip()
        issue = data.get('issue', '').strip()
        pages = data.get('pages', '').strip()
        
        return {
            'item_key': item_key,
            'title': title,
            'item_type': item_type,
            'authors': authors_str,
            'abstract': abstract,
            'publication_title': publication_title,
            'conference_name': conference_name,
            'journal_abbreviation': journal_abbreviation,
            'publisher': publisher,
            'date': date,
            'volume': volume,
            'issue': issue,
            'pages': pages,
            'doi': doi,
            'isbn': isbn,
            'issn': issn,
            'url': url,
            'tags': tags_str,
            'collections_count': len(collections),
            'collections_keys': '; '.join(collections) if collections else '',
            'date_added': date_added,
            'date_modified': date_modified,
            'abstract_length': len(abstract),
            'title_length': len(title),
            'has_doi': bool(doi),
            'has_abstract': bool(abstract),
            'has_tags': bool(tags),
        }
    
    def collect_and_save(self) -> str:
        """æ”¶é›†æ‰€æœ‰æ–‡çŒ®ä¿¡æ¯å¹¶ä¿å­˜åˆ°Excelæ–‡ä»¶"""
        print("ğŸš€ å¼€å§‹æ”¶é›†æ–‡çŒ®ä¿¡æ¯...")
        
        # è·å–æ‰€æœ‰æ–‡çŒ®
        all_items = self.get_all_items()
        if not all_items:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡çŒ®")
            return ""
        
        # å®šä¹‰çœŸæ­£çš„paperç±»å‹
        valid_paper_types = {'conferencePaper', 'document', 'journalArticle', 'preprint'}
        
        # ç­›é€‰å‡ºçœŸæ­£çš„paper
        paper_items = []
        type_counts = {}
        
        for item in all_items:
            item_type = item.get('data', {}).get('itemType', 'unknown')
            type_counts[item_type] = type_counts.get(item_type, 0) + 1
            
            if item_type in valid_paper_types:
                paper_items.append(item)
        
        print(f"ğŸ“Š æ–‡çŒ®ç±»å‹ç»Ÿè®¡:")
        for item_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            status = "âœ…" if item_type in valid_paper_types else "âŒ"
            print(f"   {status} {item_type}: {count} ç¯‡")
        
        print(f"\nğŸ¯ ç­›é€‰ç»“æœ:")
        print(f"   æ€»æ¡ç›®æ•°: {len(all_items)}")
        print(f"   çœŸæ­£paperæ•°: {len(paper_items)}")
        print(f"   è¿‡æ»¤æ‰: {len(all_items) - len(paper_items)} æ¡épaperæ¡ç›®")
        
        if not paper_items:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„paperæ–‡çŒ®")
            return ""
        
        # æå–æ–‡çŒ®ä¿¡æ¯
        print("ğŸ“Š æ­£åœ¨æå–paperè¯¦ç»†ä¿¡æ¯...")
        literature_data = []
        
        for item in tqdm(paper_items, desc="æå–ä¿¡æ¯", unit="ç¯‡"):
            try:
                info = self.extract_literature_info(item)
                literature_data.append(info)
            except Exception as e:
                print(f"æå–æ–‡çŒ®ä¿¡æ¯å¤±è´¥: {e}")
                continue
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(literature_data)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"literature_info_{timestamp}.xlsx"
        filepath = self.data_dir / filename
        
        # ä¿å­˜åˆ°Excel
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ° {filepath}...")
        df.to_excel(filepath, index=False, engine='openpyxl')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š Paperç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æœ‰æ•ˆpaperæ•°: {len(df)}")
        print(f"   paperç±»å‹åˆ†å¸ƒ:")
        type_counts = df['item_type'].value_counts()
        for item_type, count in type_counts.items():
            print(f"     âœ… {item_type}: {count} ç¯‡")
        
        print(f"   æœ‰æ‘˜è¦çš„paper: {df['has_abstract'].sum()} ç¯‡ ({df['has_abstract'].sum()/len(df)*100:.1f}%)")
        print(f"   æœ‰DOIçš„paper: {df['has_doi'].sum()} ç¯‡ ({df['has_doi'].sum()/len(df)*100:.1f}%)")
        print(f"   æœ‰æ ‡ç­¾çš„paper: {df['has_tags'].sum()} ç¯‡")
        print(f"   æœ‰åˆ†ç±»çš„paper: {(df['collections_count'] > 0).sum()} ç¯‡ ({(df['collections_count'] > 0).sum()/len(df)*100:.1f}%)")
        
        print(f"\nâœ… æ–‡çŒ®ä¿¡æ¯å·²ä¿å­˜åˆ°: {filepath}")
        return str(filepath)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“š Zoteroæ–‡çŒ®ä¿¡æ¯æ”¶é›†å·¥å…· - 001")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ['ZOTERO_USER_ID', 'ZOTERO_API_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"\nâŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("\nè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š")
        print("export ZOTERO_USER_ID='ä½ çš„Zoteroç”¨æˆ·ID'")
        print("export ZOTERO_API_KEY='ä½ çš„Zotero APIå¯†é’¥'")
        return 1
    
    try:
        # åˆ›å»ºæ”¶é›†å™¨
        collector = LiteratureCollector()
        
        # æ”¶é›†å¹¶ä¿å­˜æ–‡çŒ®ä¿¡æ¯
        result_file = collector.collect_and_save()
        
        if result_file:
            print(f"\nğŸ‰ æ”¶é›†å®Œæˆï¼")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {result_file}")
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
            print(f"   è¿è¡Œ: python 002_generate_classification_schema.py")
        else:
            print("âŒ æ”¶é›†å¤±è´¥")
            return 1
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main()) 