#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
003 - Reclassify Literature with New Schema
ä½¿ç”¨LLMå’Œæ–°çš„åˆ†ç±»ä½“ç³»å¯¹æ–‡çŒ®è¿›è¡Œæ™ºèƒ½åˆ†ç±»

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ è½½æ–°çš„åˆ†ç±»ä½“ç³»schema
2. ä½¿ç”¨LLMå¯¹æ–‡çŒ®è¿›è¡Œæ™ºèƒ½åˆ†ç±»
3. æ”¯æŒæ‰¹é‡å¤„ç†å’Œæµ‹è¯•æ¨¡å¼
4. ç”Ÿæˆåˆ†ç±»è®¡åˆ’å¹¶åº”ç”¨åˆ°Zotero

æ³¨æ„ï¼šæ­¤è„šæœ¬ä¸“æ³¨äºæ–‡çŒ®åˆ†ç±»ï¼Œé›†åˆåˆ›å»ºåœ¨005è„šæœ¬ä¸­å®ç°
"""

import os
import sys
import json
import pandas as pd
import time
import requests
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from tqdm import tqdm
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¯¼å…¥é…ç½®ç³»ç»Ÿ
from config import (
    get_llm_config, get_zotero_config, get_config,
    get_default_batch_size, get_default_test_items, get_default_max_items,
    get_max_tokens_limit, get_default_output_tokens,
    get_title_preview_length, get_description_preview_length
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from llm_client import LLMClient

class NewSchemaLiteratureClassifier:
    """åŸºäºæ–°schemaçš„æ–‡çŒ®åˆ†ç±»å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.data_dir = Path("data")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = self._init_llm_client()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_items = 0
        self.processed_items = 0
        self.successful_classifications = 0
        self.failed_classifications = 0
        
    def _init_llm_client(self) -> Optional[LLMClient]:
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿ
            return LLMClient()
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return None
    
    def _load_schema(self, schema_file: str) -> Dict[str, Any]:
        """åŠ è½½schemaæ–‡ä»¶"""
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema = json.load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½schema: {schema_file}")
            return schema
        except Exception as e:
            logger.error(f"âŒ åŠ è½½schemaå¤±è´¥: {e}")
            sys.exit(1)
    
    def _build_collection_mapping(self, schema: Dict[str, Any]) -> Dict[str, Dict[str, str]]:
        """æ„å»ºé›†åˆæ˜ å°„ï¼ˆåªåŒ…å«å¯åˆ†é…çš„å­åˆ†ç±»ï¼‰"""
        collection_mapping = {}
        
        # å§‹ç»ˆä»classification_schemaæ„å»ºï¼Œä»¥ç¡®ä¿åªåŒ…å«å­åˆ†ç±»
        classification_schema = schema.get('classification_schema', {})
        if not classification_schema:
            logger.warning("âš ï¸ schemaä¸­æœªæ‰¾åˆ° 'classification_schema'ï¼Œæ— æ³•æ„å»ºé›†åˆæ˜ å°„ã€‚")
            return {}

        main_categories = classification_schema.get('main_categories', {})
    
        for cat_code, cat_info in main_categories.items():
            # ä¸ç›´æ¥æ·»åŠ ä¸»åˆ†ç±»
            
            # å¤„ç†subcategories
            subcategories = cat_info.get('subcategories', [])
            if isinstance(subcategories, list):
                for sub_info in subcategories:
                    sub_name = sub_info.get('name', '')
                    collection_key = sub_info.get('collection_key', '')
                    sub_description = sub_info.get('description', '')
                    if sub_name and collection_key:
                        collection_mapping[collection_key] = {
                            'name': sub_name,
                            'description': sub_description
                        }
            elif isinstance(subcategories, dict):
                for sub_code, sub_info in subcategories.items():
                    sub_name = sub_info.get('name', '')
                    sub_description = sub_info.get('description', '')
                    if sub_name:
                        collection_mapping[sub_code] = {
                            'name': sub_name,
                            'description': sub_description
                        }
        
        logger.info(f"âœ… æ„å»ºä»…åŒ…å«å­åˆ†ç±»çš„é›†åˆæ˜ å°„: {len(collection_mapping)} ä¸ªé›†åˆ")
        return collection_mapping
    
    def _load_literature_data(self, literature_file: str) -> List[Dict[str, Any]]:
        """åŠ è½½æ–‡çŒ®æ•°æ®"""
        try:
            # æ”¯æŒExcelå’ŒJSONæ ¼å¼
            if literature_file.endswith('.xlsx'):
                df = pd.read_excel(literature_file)
                literature_data = df.to_dict('records')
            elif literature_file.endswith('.json'):
                with open(literature_file, 'r', encoding='utf-8') as f:
                    literature_data = json.load(f)
            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {literature_file}")
                return []
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ–‡çŒ®æ•°æ®: {len(literature_data)} ç¯‡æ–‡çŒ®")
            return literature_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡çŒ®æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _prepare_classification_prompt(self, item: Dict[str, Any], collection_mapping: Dict[str, Dict[str, str]]) -> str:
        """å‡†å¤‡åˆ†ç±»æç¤ºè¯"""
        title = str(item.get('title', '')).strip()
        abstract = str(item.get('abstract', '')).strip()
        
        # æ„å»ºé›†åˆåˆ—è¡¨
        collection_list = []
        for code, info in collection_mapping.items():
            name = info.get('name', '')
            description = info.get('description', '')
            collection_list.append(f"- {code}: {name} - {description}")
        
        collections_text = "\n".join(collection_list)
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ–‡çŒ®ä¿¡æ¯ï¼Œä»ç»™å®šçš„é›†åˆä¸­é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»ã€‚

æ–‡çŒ®ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{abstract}

å¯ç”¨é›†åˆï¼š
{collections_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†ç±»ç»“æœï¼š
{{
    "recommended_collections": ["collection_code1", "collection_code2"],
    "reasoning": "åˆ†ç±»ç†ç”±è¯´æ˜"
}}

è¦æ±‚ï¼š
1. recommended_collections: æœ€å¤šæ¨è5ä¸ªé›†åˆï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
2. åªä½¿ç”¨ä¸Šè¿°é›†åˆä»£ç ï¼Œä¸è¦åˆ›å»ºæ–°çš„åˆ†ç±»
3. ç¡®ä¿é›†åˆä»£ç å®Œå…¨åŒ¹é…
4. reasoning: ç®€è¦è¯´æ˜åˆ†ç±»ç†ç”±
5. å¦‚æœæ–‡çŒ®ä¸ä»»ä½•é›†åˆéƒ½ä¸åŒ¹é…ï¼Œè¿”å›ç©ºæ•°ç»„

è¯·åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""
        
        return prompt
    
    def _parse_classification_response(self, response: str) -> Dict[str, Any]:
        """è§£æåˆ†ç±»å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                return {"recommended_collections": [], "reasoning": "æ— æ³•è§£æå“åº”"}
            
            json_str = response[start_idx:end_idx]
            result = json.loads(json_str)
            
            # éªŒè¯å“åº”æ ¼å¼
            if 'recommended_collections' not in result:
                result['recommended_collections'] = []
            if 'reasoning' not in result:
                result['reasoning'] = "æœªæä¾›åˆ†ç±»ç†ç”±"
            
            return result
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSONè§£æå¤±è´¥: {e}")
            return {"recommended_collections": [], "reasoning": f"JSONè§£æå¤±è´¥: {e}"}
        except Exception as e:
            logger.warning(f"è§£æå“åº”å¤±è´¥: {e}")
            return {"recommended_collections": [], "reasoning": f"è§£æå¤±è´¥: {e}"}
    
    def _prepare_batch_classification_prompt(self, items: List[Dict[str, Any]], collection_mapping: Dict[str, Dict[str, str]]) -> str:
        """å‡†å¤‡æ‰¹é‡åˆ†ç±»æç¤ºè¯"""
        # æ„å»ºé›†åˆåˆ—è¡¨
        collection_list = []
        for code, info in collection_mapping.items():
            name = info.get('name', '')
            description = info.get('description', '')
            collection_list.append(f"- {code}: {name} - {description}")
        
        collections_text = "\n".join(collection_list)
        
        # æ„å»ºæ–‡çŒ®åˆ—è¡¨
        items_text = ""
        for i, item in enumerate(items, 1):
            title = str(item.get('title', '')).strip()
            abstract = str(item.get('abstract', '')).strip()
            item_key = item.get('item_key', '')
            
            items_text += f"""
æ–‡çŒ® {i} (ID: {item_key}):
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{abstract}
"""
        
        prompt = f"""
# ROLE: You are a professional AI literature classification engine.

# CORE TASK: Your primary task is to accurately assign each document from a given list (`items_text`) to one or more relevant categories from a predefined, flat list of collections (`collections_text`).

---

### Input Data

1.  **Available Collections (`{collections_text}`)**:
    * **Format**: A flat JSON list of available classification categories. Each category object contains a `collection_key`,`name`,`description`.
    * **Example**: `- 9KGVHHUD: Foundation Models - Large-scale models pre-trained on vast data, serving as a base for various downstream tasks, such as GPT-3, Llama 3, and ERNIE 4.5.\n- T6PHSH3J: Large Language Models (LLMs) - Models specifically designed for understanding, generating, and processing natural language, including architectures, training methodologies, and few-shot learning capabilities.`

2.  **Items to Classify (`{items_text}`)**:
    * **Format**: A JSON list of documents, where each document has a unique `literature`, `title` and `abstract`.

---

### Core Requirements

1.  **High-Confidence Principle: DO NOT FORCE CLASSIFICATION.** You must first carefully read and internalize the `description` of each available category. Only recommend a category if the document's core topic **clearly and strongly aligns** with the category's description. Avoid all weak, speculative, or overly broad matches.
2.  **Semantic Matching**: Perform a precise semantic match between a document's primary research contribution and the category descriptions. Focus on the core problem being solved, not just shared keywords.
3.  **Ranking and Limits**: If confident matches are found, recommend **1 to 5** of the most relevant categories. The results in the `recommended_collections` array MUST be sorted by relevance, from **highest to lowest**.
4.  **Code Integrity**: The `recommended_collections` array MUST ONLY contain the `code` values from the provided `collections_text` list. Ensure the codes match exactly.
5.  **Provide Reasoning**: In the `reasoning` field, provide a brief, one-sentence explanation that justifies the high-confidence match by linking the document's specific contribution to the category's definition.
6.  **No-Match Handling**: Following the High-Confidence Principle, if a document does not have a strong and clear match with any category, the `recommended_collections` field must be an **empty array `[]`**.
7.  **Maintain Order**: The order of the documents in your final output must be **exactly the same** as the order in the input `items_text`.

---

### Output Format

You MUST strictly adhere to the following JSON structure. Do not include any text, notes, or explanations outside of this JSON object.

```json
{{
    "classifications": [
        {{
            "item_key": "item_key_of_document_1",
            "recommended_collections": ["collection_code1", "collection_code2"],
            "reasoning": "A brief explanation of why the document was assigned to these collections."
        }},
        {{
            "item_key": "item_key_of_document_2",
            "recommended_collections": ["collection_code3"],
            "reasoning": "A brief explanation of why the document was assigned to this collection."
        }}
    ]
}}
```
"""

        return prompt
    
    def _parse_batch_classification_response(self, response: str, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è§£ææ‰¹é‡åˆ†ç±»å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                # å¦‚æœæ— æ³•è§£æï¼Œä¸ºæ‰€æœ‰æ–‡çŒ®è¿”å›å¤±è´¥ç»“æœ
                return [{
                    'item_key': item.get('item_key', ''),
                    'title': item.get('title', ''),
                    'classification_success': False,
                    'recommended_collections': [],
                    'reasoning': 'æ— æ³•è§£æå“åº”',
                    'error_message': 'å“åº”æ ¼å¼é”™è¯¯'
                } for item in items]
            
            json_str = response[start_idx:end_idx]
            result = json.loads(json_str)
            
            # éªŒè¯å“åº”æ ¼å¼
            if 'classifications' not in result:
                # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œä¸ºæ‰€æœ‰æ–‡çŒ®è¿”å›å¤±è´¥ç»“æœ
                return [{
                    'item_key': item.get('item_key', ''),
                    'title': item.get('title', ''),
                    'classification_success': False,
                    'recommended_collections': [],
                    'reasoning': 'å“åº”æ ¼å¼é”™è¯¯',
                    'error_message': 'ç¼ºå°‘classificationså­—æ®µ'
                } for item in items]
            
            classifications = result['classifications']
            results = []
            
            # ä¸ºæ¯ä¸ªæ–‡çŒ®åˆ›å»ºç»“æœ
            for i, item in enumerate(items):
                item_key = item.get('item_key', '')
                
                # æŸ¥æ‰¾å¯¹åº”çš„åˆ†ç±»ç»“æœ
                classification = None
                for cls in classifications:
                    if cls.get('item_key') == item_key:
                        classification = cls
                        break
                
                if classification and 'recommended_collections' in classification:
                    results.append({
                        'item_key': item_key,
                        'title': item.get('title', ''),
                        'classification_success': len(classification['recommended_collections']) > 0,
                        'recommended_collections': classification.get('recommended_collections', []),
                        'reasoning': classification.get('reasoning', ''),
                        'error_message': '' if len(classification['recommended_collections']) > 0 else 'æœªæ‰¾åˆ°åˆé€‚çš„åˆ†ç±»'
                    })
                else:
                    results.append({
                        'item_key': item_key,
                        'title': item.get('title', ''),
                        'classification_success': False,
                        'recommended_collections': [],
                        'reasoning': 'æœªæ‰¾åˆ°å¯¹åº”çš„åˆ†ç±»ç»“æœ',
                        'error_message': 'å“åº”ä¸­ç¼ºå°‘è¯¥æ–‡çŒ®çš„åˆ†ç±»ä¿¡æ¯'
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"è§£ææ‰¹é‡åˆ†ç±»å“åº”å¤±è´¥: {e}")
            # è¿”å›å¤±è´¥ç»“æœ
            return [{
                'item_key': item.get('item_key', ''),
                'title': item.get('title', ''),
                'classification_success': False,
                'recommended_collections': [],
                'reasoning': f'è§£æå¤±è´¥: {str(e)}',
                'error_message': 'å“åº”è§£æå¼‚å¸¸'
            } for item in items]

    def _classify_batch(self, items: List[Dict[str, Any]], collection_mapping: Dict[str, Dict[str, str]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ç±»æ–‡çŒ®"""
        if not self.llm_client:
            logger.error("âŒ LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return [{
                'item_key': item.get('item_key', ''),
                'title': item.get('title', ''),
                'classification_success': False,
                'recommended_collections': [],
                'reasoning': '',
                'error_message': 'LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–'
            } for item in items]
        
        try:
            # å‡†å¤‡æ‰¹é‡åˆ†ç±»æç¤ºè¯
            prompt = self._prepare_batch_classification_prompt(items, collection_mapping)
            
            # è°ƒç”¨LLM API
            response = self.llm_client.generate_text(prompt)
            
            if not response:
                logger.error("âŒ LLM APIè¿”å›ç©ºå“åº”")
                return [{
                    'item_key': item.get('item_key', ''),
                    'title': item.get('title', ''),
                    'classification_success': False,
                    'recommended_collections': [],
                    'reasoning': '',
                    'error_message': 'LLM APIè¿”å›ç©ºå“åº”'
                } for item in items]
            
            # è§£æå“åº”
            results = self._parse_batch_classification_response(response, items)
            
            # ç»Ÿè®¡æˆåŠŸæ•°é‡
            successful = sum(1 for r in results if r['classification_success'])
            logger.info(f"ğŸ“Š æ‰¹é‡åˆ†ç±»å®Œæˆ: {len(items)} ç¯‡, æˆåŠŸ: {successful} ç¯‡")
            
            return results
    
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†ç±»å¤±è´¥: {e}")
            return [{
                'item_key': item.get('item_key', ''),
                'title': item.get('title', ''),
                'classification_success': False,
                'recommended_collections': [],
                'reasoning': '',
                'error_message': str(e)
            } for item in items]
    
    def classify_literature(self, schema_file: str, literature_file: str, max_items: int = None, batch_size: int = None) -> str:
        """å¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±»"""
        # åŠ è½½schemaå’Œé›†åˆæ˜ å°„
        schema = self._load_schema(schema_file)
        collection_mapping = self._build_collection_mapping(schema)
        
        # åŠ è½½æ–‡çŒ®æ•°æ®
        literature_data = self._load_literature_data(literature_file)
        if not literature_data:
            logger.error("âŒ æ²¡æœ‰å¯åˆ†ç±»çš„æ–‡çŒ®æ•°æ®")
            return ""
        
        # é™åˆ¶å¤„ç†æ•°é‡
        if max_items:
            literature_data = literature_data[:max_items]
        
        self.total_items = len(literature_data)
        logger.info(f"ğŸ“Š å¼€å§‹åˆ†ç±» {self.total_items} ç¯‡æ–‡çŒ®")
        
        # æ‰¹é‡å¤„ç†
        batch_size = batch_size or get_default_batch_size()
        results = []
        
        for i in range(0, len(literature_data), batch_size):
            batch = literature_data[i:i + batch_size]
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(literature_data) + batch_size - 1)//batch_size}")
            
            batch_results = self._classify_batch(batch, collection_mapping)
            results.extend(batch_results)
            
            # ç»Ÿè®¡è¿›åº¦
            successful = sum(1 for r in batch_results if r['classification_success'])
            logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: {len(batch_results)} ç¯‡, æˆåŠŸ: {successful} ç¯‡")
        
        # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/classification_plan_{timestamp}.json"
        excel_file = f"data/classification_plan_{timestamp}.xlsx"
        
        output_data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'schema_file': schema_file,
                'literature_file': literature_file,
                'total_items': len(results),
                'successful_classifications': sum(1 for r in results if r['classification_success']),
                'failed_classifications': sum(1 for r in results if not r['classification_success'])
            },
            'classifications': results
        }
        
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
        
            logger.info(f"âœ… åˆ†ç±»è®¡åˆ’å·²ä¿å­˜åˆ°: {output_file}")
            
            # ç”ŸæˆExcelæ–‡ä»¶
            self._save_excel_report(results, excel_file, collection_mapping, literature_file)
            logger.info(f"âœ… ExcelæŠ¥å‘Šå·²ä¿å­˜åˆ°: {excel_file}")
            
            return output_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†ç±»è®¡åˆ’å¤±è´¥: {e}")
            return ""
        
    def _save_excel_report(self, results: List[Dict[str, Any]], excel_file: str, collection_mapping: Dict[str, Dict[str, str]], literature_file: str) -> None:
        """ä¿å­˜Excelæ ¼å¼çš„åˆ†ç±»æŠ¥å‘Š"""
        try:
            import pandas as pd
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
            from openpyxl.utils.dataframe import dataframe_to_rows
            
            # è¯»å–åŸå§‹æ–‡çŒ®æ•°æ®
            original_data = self._load_literature_data(literature_file)
            
            # åˆ›å»ºç»“æœæ˜ å°„å­—å…¸
            results_dict = {result['item_key']: result for result in results}
            
            # ä¸ºæ¯ä¸ªåŸå§‹æ–‡çŒ®æ·»åŠ æ–°çš„åˆ†ç±»ç»“æœ
            excel_data = []
            for item in original_data:
                item_key = item.get('item_key', '')
                result = results_dict.get(item_key, {})
                
                # å°†æ¨èé›†åˆåˆ—è¡¨è½¬æ¢ä¸ºå¯è¯»çš„æ–‡æœ¬
                recommended_collections = result.get('recommended_collections', [])
                collections_text = ""
                if recommended_collections:
                    collection_names = []
                    for code in recommended_collections:
                        info = collection_mapping.get(code, {})
                        name = info.get('name', code) if isinstance(info, dict) else code
                        collection_names.append(f"{code}: {name}")
                    collections_text = "; ".join(collection_names)
                
                # åˆ›å»ºè¡Œæ•°æ®ï¼Œä¿æŒåŸæœ‰æ ¼å¼
                row_data = {
                    'item_key': item.get('item_key', ''),
                    'title': item.get('title', ''),
                    'item_type': item.get('item_type', ''),
                    'authors': item.get('authors', ''),
                    'publication_title': item.get('publication_title', ''),
                    'conference_name': item.get('conference_name', ''),
                    'date': item.get('date', ''),
                    'doi': item.get('doi', ''),
                    'abstract': item.get('abstract', ''),
                    'tags': item.get('tags', ''),
                    'url': item.get('url', ''),
                    'language': item.get('language', ''),
                    'pages': item.get('pages', ''),
                    'volume': item.get('volume', ''),
                    'issue': item.get('issue', ''),
                    'publisher': item.get('publisher', ''),
                    'place': item.get('place', ''),
                    'edition': item.get('edition', ''),
                    'series': item.get('series', ''),
                    'isbn': item.get('isbn', ''),
                    'issn': item.get('issn', ''),
                    'call_number': item.get('call_number', ''),
                    'access_date': item.get('access_date', ''),
                    'rights': item.get('rights', ''),
                    'extra': item.get('extra', ''),
                    'collections': item.get('collections', ''),
                    'collections_keys': item.get('collections_keys', ''),
                    'collections_count': item.get('collections_count', 0),
                    'notes': item.get('notes', ''),
                    'attachments': item.get('attachments', ''),
                    'attachments_count': item.get('attachments_count', 0),
                    'related_items': item.get('related_items', ''),
                    'related_items_count': item.get('related_items_count', 0),
                    'created_date': item.get('created_date', ''),
                    'modified_date': item.get('modified_date', ''),
                    'last_modified_by': item.get('last_modified_by', ''),
                    'version': item.get('version', ''),
                    # æ–°å¢åˆ†ç±»ç»“æœåˆ—
                    'new_classification_success': result.get('classification_success', False),
                    'new_recommended_collection_keys': '; '.join(recommended_collections) if recommended_collections else '',
                    'new_recommended_collections': collections_text,
                    'new_recommended_count': len(recommended_collections),
                    'new_analysis': result.get('reasoning', ''),
                    'new_error_message': result.get('error_message', ''),
                    'new_worker_id': '006_reclassify_with_new_schema',
                    'new_response': '',  # å¯ä»¥æ·»åŠ åŸå§‹å“åº”
                    'new_classification_timestamp': datetime.now().strftime("%Y%m%d_%H%M%S")
                }
                excel_data.append(row_data)
        
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(excel_data)
        
            # åˆ›å»ºExcelæ–‡ä»¶
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # å†™å…¥ä¸»æ•°æ®è¡¨
                df.to_excel(writer, sheet_name='åˆ†ç±»ç»“æœ', index=False)
                
                # è·å–å·¥ä½œè¡¨å¯¹è±¡
                worksheet = writer.sheets['åˆ†ç±»ç»“æœ']
                
                # è®¾ç½®åˆ—å®½
                column_widths = {
                    'A': 15,  # item_key
                    'B': 50,  # title
                    'C': 15,  # item_type
                    'D': 30,  # authors
                    'E': 40,  # publication_title
                    'F': 30,  # conference_name
                    'G': 15,  # date
                    'H': 25,  # doi
                    'I': 60,  # abstract
                    'J': 30,  # tags
                    'K': 40,  # url
                    'L': 10,  # language
                    'M': 10,  # pages
                    'N': 10,  # volume
                    'O': 10,  # issue
                    'P': 25,  # publisher
                    'Q': 20,  # place
                    'R': 15,  # edition
                    'S': 20,  # series
                    'T': 20,  # isbn
                    'U': 15,  # issn
                    'V': 20,  # call_number
                    'W': 15,  # access_date
                    'X': 20,  # rights
                    'Y': 30,  # extra
                    'Z': 40,  # collections
                    'AA': 40, # collections_keys
                    'AB': 15, # collections_count
                    'AC': 40, # notes
                    'AD': 40, # attachments
                    'AE': 15, # attachments_count
                    'AF': 40, # related_items
                    'AG': 15, # related_items_count
                    'AH': 20, # created_date
                    'AI': 20, # modified_date
                    'AJ': 20, # last_modified_by
                    'AK': 10, # version
                    'AL': 20, # new_classification_success
                    'AM': 40, # new_recommended_collection_keys
                    'AN': 60, # new_recommended_collections
                    'AO': 15, # new_recommended_count
                    'AP': 60, # new_analysis
                    'AQ': 30, # new_error_message
                    'AR': 25, # new_worker_id
                    'AS': 40, # new_response
                    'AT': 20  # new_classification_timestamp
                }
                
                for col, width in column_widths.items():
                    worksheet.column_dimensions[col].width = width
                
                # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_alignment = Alignment(horizontal="center", vertical="center")
                
                for cell in worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = header_alignment
                
                # è®¾ç½®æ•°æ®è¡Œæ ·å¼
                success_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # æµ…ç»¿è‰²
                failure_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")  # æµ…çº¢è‰²
                
                for row_idx, row in enumerate(worksheet.iter_rows(min_row=2), start=2):
                    # æ ¹æ®åˆ†ç±»æˆåŠŸçŠ¶æ€è®¾ç½®èƒŒæ™¯è‰²ï¼ˆåªå¯¹æ–°åˆ†ç±»åˆ—è®¾ç½®èƒŒæ™¯è‰²ï¼‰
                    success_cell = row[37]  # new_classification_successåˆ— (ALåˆ—ï¼Œç´¢å¼•37)
                    if success_cell.value == True:
                        # åªå¯¹æ–°åˆ†ç±»ç›¸å…³çš„åˆ—è®¾ç½®ç»¿è‰²èƒŒæ™¯
                        for i in range(37, 46):  # ALåˆ°ATåˆ—
                            if i < len(row):
                                row[i].fill = success_fill
                    else:
                        # åªå¯¹æ–°åˆ†ç±»ç›¸å…³çš„åˆ—è®¾ç½®çº¢è‰²èƒŒæ™¯
                        for i in range(37, 46):  # ALåˆ°ATåˆ—
                            if i < len(row):
                                row[i].fill = failure_fill
                    
                    # è®¾ç½®æ–‡æœ¬æ¢è¡Œ
                    for cell in row:
                        cell.alignment = Alignment(wrap_text=True, vertical="top")
        
                # åˆ›å»ºç»Ÿè®¡ä¿¡æ¯è¡¨
                stats_data = {
                    'ç»Ÿè®¡é¡¹ç›®': [
                        'æ€»æ–‡çŒ®æ•°',
                        'åˆ†ç±»æˆåŠŸæ•°',
                        'åˆ†ç±»å¤±è´¥æ•°',
                        'æˆåŠŸç‡',
                        'ç”Ÿæˆæ—¶é—´'
                    ],
                    'æ•°å€¼': [
                        len(results),
                        sum(1 for r in results if r['classification_success']),
                        sum(1 for r in results if not r['classification_success']),
                        f"{sum(1 for r in results if r['classification_success']) / len(results) * 100:.1f}%" if results else "0%",
                        datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    ]
                }
                
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
                
                # è®¾ç½®ç»Ÿè®¡è¡¨æ ·å¼
                stats_worksheet = writer.sheets['ç»Ÿè®¡ä¿¡æ¯']
                stats_worksheet.column_dimensions['A'].width = 15
                stats_worksheet.column_dimensions['B'].width = 20
                
                for cell in stats_worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = header_alignment
                
                # åˆ›å»ºé›†åˆæ˜ å°„è¡¨
                mapping_data = []
                for code, info in collection_mapping.items():
                    name = info.get('name', '') if isinstance(info, dict) else str(info)
                    description = info.get('description', '') if isinstance(info, dict) else ''
                    mapping_data.append({
                        'é›†åˆä»£ç ': code,
                        'é›†åˆåç§°': name,
                        'é›†åˆæè¿°': description
                    })
                
                mapping_df = pd.DataFrame(mapping_data)
                mapping_df.to_excel(writer, sheet_name='é›†åˆæ˜ å°„', index=False)
                
                # è®¾ç½®æ˜ å°„è¡¨æ ·å¼
                mapping_worksheet = writer.sheets['é›†åˆæ˜ å°„']
                mapping_worksheet.column_dimensions['A'].width = 20
                mapping_worksheet.column_dimensions['B'].width = 40
                mapping_worksheet.column_dimensions['C'].width = 60
                
                for cell in mapping_worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = header_alignment
                
        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•ç”ŸæˆExcelæ–‡ä»¶ï¼Œç¼ºå°‘ä¾èµ–: {e}")
            logger.info("è¯·å®‰è£…: pip install pandas openpyxl")
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆExcelæ–‡ä»¶å¤±è´¥: {e}")
        

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="003 - ä½¿ç”¨LLMå’Œæ–°çš„åˆ†ç±»Schemaå¯¹æ–‡çŒ®è¿›è¡Œæ™ºèƒ½åˆ†ç±»ï¼Œç”Ÿæˆåˆ†ç±»è®¡åˆ’",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ç”Ÿæˆåˆ†ç±»è®¡åˆ’
  python 006_reclassify_with_new_schema.py --plan --schema data/schema_with_collection_keys.json --input data/literature_info.xlsx
  
  # æµ‹è¯•æ¨¡å¼
  python 006_reclassify_with_new_schema.py --test --schema data/schema_with_collection_keys.json --input data/literature_info.xlsx
  
  # æŒ‡å®šæ‰¹é‡å¤§å°
  python 006_reclassify_with_new_schema.py --plan --schema data/schema_with_collection_keys.json --input data/literature_info.xlsx --batch-size 25

æ³¨æ„äº‹é¡¹:
  - éœ€è¦é…ç½®LLM APIç¯å¢ƒå˜é‡
  - å»ºè®®å…ˆä½¿ç”¨--testæ¨¡å¼æµ‹è¯•
  - åˆ†ç±»ç»“æœéœ€è¦æ‰‹åŠ¨åº”ç”¨åˆ°Zotero
        """
    )
    
    # åˆ›å»ºäº’æ–¥ç»„
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨è¾ƒå°‘æ•°æ®ï¼‰')
    mode_group.add_argument('--plan', action='store_true', help='ç”Ÿæˆåˆ†ç±»è®¡åˆ’')
    
    # æ–‡ä»¶è·¯å¾„å‚æ•°ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰
    parser.add_argument('--schema', type=str, required=True, help='åˆ†ç±»schemaæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('--input', type=str, required=True, help='æ–‡çŒ®æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆExcelæˆ–JSONæ ¼å¼ï¼‰')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--max-items', type=int, help='æœ€å¤§å¤„ç†æ–‡çŒ®æ•°é‡')
    parser.add_argument('--batch-size', type=int, help='æ‰¹é‡å¤„ç†å¤§å°')
    
    args = parser.parse_args()
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(args.schema):
        parser.error(f"Schemaæ–‡ä»¶ä¸å­˜åœ¨: {args.schema}")
    if not os.path.exists(args.input):
        parser.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
    
        # åˆ›å»ºåˆ†ç±»å™¨
    classifier = NewSchemaLiteratureClassifier()
        
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
    if args.test:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {args.max_items or get_default_test_items()} ç¯‡æ–‡çŒ®è¿›è¡Œæµ‹è¯•...")
        max_items = args.max_items or get_default_test_items()
    else:
        print(f"\nğŸ“ ç”Ÿæˆåˆ†ç±»è®¡åˆ’...")
        max_items = args.max_items
    
    # æ‰§è¡Œåˆ†ç±»
    result_file = classifier.classify_literature(
        schema_file=args.schema,
        literature_file=args.input,
        max_items=max_items,
        batch_size=args.batch_size
    )
            
    if result_file:
        # ç”Ÿæˆå¯¹åº”çš„Excelæ–‡ä»¶å
        excel_file = result_file.replace('.json', '.xlsx')
        print(f"\nâœ… åˆ†ç±»å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"  ğŸ“„ JSONæ ¼å¼: {result_file}")
        print(f"  ğŸ“Š Excelæ ¼å¼: {excel_file}")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"  1. æŸ¥çœ‹ExcelæŠ¥å‘Š: {excel_file}")
        print(f"  2. æ£€æŸ¥JSONæ•°æ®: {result_file}")
        print(f"  3. åº”ç”¨åˆ†ç±»åˆ°Zotero:")
        print(f"     python 005_apply_classification_to_zotero.py --plan {result_file} --test")
        return 0
    else:
        print("âŒ åˆ†ç±»å¤±è´¥")
        return 1
        

if __name__ == "__main__":
    sys.exit(main()) 