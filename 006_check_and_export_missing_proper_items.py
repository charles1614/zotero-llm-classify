#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
006 - æ£€æŸ¥å¹¶å¯¼å‡ºæœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®
æ£€æŸ¥å¹¶å¯¼å‡ºæœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®

ä¸»è¦åŠŸèƒ½ï¼š
1. æ£€æŸ¥Zoteroä¸­æœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®
2. å¯¼å‡ºæœªåˆ†ç±»æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯
3. æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆJSONã€Excelï¼‰
4. è¿‡æ»¤éæ ‡å‡†æ–‡çŒ®ç±»å‹
5. é«˜æ€§èƒ½æ‰¹é‡å¤„ç†

æ³¨æ„ï¼šæ­¤è„šæœ¬åªå¤„ç†æ ‡å‡†çš„Zoteroæ–‡çŒ®ç±»å‹ï¼Œä¸åŒ…æ‹¬é™„ä»¶ã€ç¬”è®°ç­‰
"""

import os
import sys
import json
import argparse
import requests
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# å¯¼å…¥é…ç½®ç³»ç»Ÿ
from config import (
    get_zotero_config, get_config,
    get_default_limit, get_abstract_limit
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MissingItemsChecker:
    """æœªåˆ†ç±»æ–‡çŒ®æ£€æŸ¥å™¨"""
    
    def __init__(self, abstract_limit: int = None, schema_file: str = None):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.zotero_config = get_zotero_config()
        self.base_url = self.zotero_config.api_base_url
        self.user_id = self.zotero_config.user_id
        self.headers = self.zotero_config.headers
        
        # ç¼“å­˜é›†åˆä¿¡æ¯
        self._collections_cache = None
        self._collections_cache_time = 0
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        # æ‘˜è¦é•¿åº¦é™åˆ¶
        self.abstract_limit = abstract_limit or get_abstract_limit()
        
        # åŠ è½½schemaæ–‡ä»¶
        self.schema_collection_keys = set()
        if schema_file:
            self._load_schema_collection_keys(schema_file)
        
        # æ ‡å‡†æ–‡çŒ®ç±»å‹ï¼ˆæ’é™¤é™„ä»¶ã€ç¬”è®°ç­‰ï¼‰
        self.proper_item_types = {
            'journalArticle', 'conferencePaper', 'book', 'bookSection', 
            'thesis', 'report', 'document', 'preprint', 'patent',
            'webpage', 'computerProgram', 'software', 'dataset',
            'presentation', 'videoRecording', 'audioRecording',
            'artwork', 'map', 'blogPost', 'forumPost', 'email',
            'letter', 'manuscript', 'encyclopediaArticle', 'dictionaryEntry',
            'newspaperArticle', 'magazineArticle', 'case', 'statute',
            'hearing', 'bill', 'treaty', 'regulation', 'standard'
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_items = 0
        self.proper_items = 0
        self.unfiled_items = 0
        self.exported_items = 0
        
    def _load_schema_collection_keys(self, schema_file: str):
        """åŠ è½½schemaæ–‡ä»¶ä¸­çš„collection_key"""
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_data = json.load(f)
            
            # æå–æ‰€æœ‰collection_key
            collection_keys = set()
            
            # ä»ä¸»åˆ†ç±»ä¸­æå–
            main_categories = schema_data.get('classification_schema', {}).get('main_categories', {})
            for category in main_categories.values():
                if 'collection_key' in category:
                    collection_keys.add(category['collection_key'])
                
                # ä»å­åˆ†ç±»ä¸­æå–
                subcategories = category.get('subcategories', [])
                for subcategory in subcategories:
                    if 'collection_key' in subcategory:
                        collection_keys.add(subcategory['collection_key'])
            
            # ä»ç‹¬ç«‹åˆ†ç±»ä¸­æå–
            independent_categories = schema_data.get('classification_schema', {}).get('independent_categories', {})
            for category in independent_categories.values():
                if 'collection_key' in category:
                    collection_keys.add(category['collection_key'])
            
            self.schema_collection_keys = collection_keys
            logger.info(f"âœ… å·²åŠ è½½schemaæ–‡ä»¶ï¼ŒåŒ…å« {len(collection_keys)} ä¸ªåˆ†ç±»é›†åˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½schemaæ–‡ä»¶å¤±è´¥: {e}")
            self.schema_collection_keys = set()
    
    def _get_all_items(self, limit: int = None) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ–‡çŒ®é¡¹ç›®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        all_items = []
        start = 0
        batch_size = min(limit or get_default_limit(), 100)  # é™åˆ¶æ‰¹é‡å¤§å°
        
        logger.info(f"ğŸ“Š å¼€å§‹è·å–æ–‡çŒ®é¡¹ç›® (æ‰¹é‡å¤§å°: {batch_size})...")
        
        while True:
            try:
                url = f"{self.base_url}/items"
                params = {
                    'start': start,
                    'limit': batch_size,
                    'format': 'json'
                }
            
                response = requests.get(url, headers=self.headers, params=params, timeout=30)
                response.raise_for_status()
                
                items = response.json()
                if not items:
                    break
                
                all_items.extend(items)
                start += len(items)
                
                if len(all_items) % 500 == 0:
                    logger.info(f"ğŸ“¦ å·²è·å– {len(all_items)} ä¸ªé¡¹ç›®...")
                
                # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œåœæ­¢
                if limit and len(all_items) >= limit:
                    all_items = all_items[:limit]
                    break
                    
                # å¦‚æœè¿”å›çš„é¡¹ç›®æ•°å°‘äºæ‰¹é‡å¤§å°ï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰é¡¹ç›®
                if len(items) < batch_size:
                    break
                
            except Exception as e:
                logger.error(f"âŒ è·å–æ–‡çŒ®é¡¹ç›®å¤±è´¥: {e}")
                break
        
        logger.info(f"âœ… æ€»å…±è·å–åˆ° {len(all_items)} ä¸ªé¡¹ç›®")
        return all_items
    
    def _get_all_collections(self) -> Dict[str, str]:
        """è·å–æ‰€æœ‰é›†åˆçš„keyåˆ°nameçš„æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if (self._collections_cache is not None and 
            current_time - self._collections_cache_time < self._cache_ttl):
            return self._collections_cache
        
        try:
            url = f"{self.base_url}/collections"
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            collections = response.json()
            collection_dict = {}
            
            for collection in collections:
                key = collection.get('key')
                name = collection.get('data', {}).get('name', '')
                if key and name:
                    collection_dict[key] = name
            
            # æ›´æ–°ç¼“å­˜
            self._collections_cache = collection_dict
            self._collections_cache_time = current_time
            
            return collection_dict
            
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _is_proper_item(self, item: Dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡å‡†æ–‡çŒ®é¡¹ç›®"""
        item_type = item.get('data', {}).get('itemType', '')
        return item_type in self.proper_item_types
    
    def _needs_classification(self, item: Dict[str, Any]) -> bool:
        """åˆ¤æ–­é¡¹ç›®æ˜¯å¦éœ€è¦åˆ†ç±»"""
        collections = item.get('data', {}).get('collections', [])
        
        # å¦‚æœæ²¡æœ‰é›†åˆï¼Œéœ€è¦åˆ†ç±»
        if len(collections) == 0:
            return True
        
        # å¦‚æœæ²¡æœ‰åŠ è½½schemaæ–‡ä»¶ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
        if not self.schema_collection_keys:
            return False
        
        # æ£€æŸ¥æ–‡çŒ®çš„é›†åˆæ˜¯å¦åœ¨schemaä¸­
        for collection_key in collections:
            if collection_key in self.schema_collection_keys:
                # å¦‚æœæ–‡çŒ®åœ¨schemaä¸­çš„é›†åˆé‡Œï¼Œè¯´æ˜å·²åˆ†ç±»
                return False
        
        # å¦‚æœæ–‡çŒ®çš„é›†åˆéƒ½ä¸åœ¨schemaä¸­ï¼Œè¯´æ˜æœªåˆ†ç±»
        return True
    
    def _get_item_details_batch(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰"""
        if not items:
            return []
        
        logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡è·å– {len(items)} ä¸ªé¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        max_workers = min(10, len(items))  # é™åˆ¶å¹¶å‘æ•°
        detailed_items = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_item = {
                executor.submit(self._get_single_item_details, item): item 
                for item in items
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in as_completed(future_to_item):
                try:
                    details = future.result()
                    if details:
                        detailed_items.append(details)
                    completed += 1
                    
                    if completed % 50 == 0:
                        logger.info(f"ğŸ“‹ å·²å¤„ç† {completed}/{len(items)} ä¸ªé¡¹ç›®...")
                        
                except Exception as e:
                    item = future_to_item[future]
                    logger.warning(f"âš ï¸  å¤„ç†é¡¹ç›®å¤±è´¥ {item.get('key', 'unknown')}: {e}")
                    completed += 1
        
        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸè·å– {len(detailed_items)} ä¸ªé¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯")
        return detailed_items
    
    def _get_single_item_details(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è·å–å•ä¸ªæ–‡çŒ®è¯¦ç»†ä¿¡æ¯"""
        try:
            item_data = item.get('data', {})
            
            # åŸºæœ¬ä¿¡æ¯
            details = {
                'item_key': item.get('key', ''),
                'title': item_data.get('title', ''),
                'item_type': item_data.get('itemType', ''),
                'authors': self._extract_authors(item_data),
                'publication_title': item_data.get('publicationTitle', ''),
                'conference_name': item_data.get('conferenceName', ''),
                'date': item_data.get('date', ''),
                'doi': item_data.get('DOI', ''),
                'abstract': self._extract_abstract(item_data),
                'tags': self._extract_tags(item_data),
                'url': item_data.get('url', ''),
                'language': item_data.get('language', ''),
                'pages': item_data.get('pages', ''),
                'volume': item_data.get('volume', ''),
                'issue': item_data.get('issue', ''),
                'publisher': item_data.get('publisher', ''),
                'place': item_data.get('place', ''),
                'edition': item_data.get('edition', ''),
                'series': item_data.get('series', ''),
                'isbn': item_data.get('ISBN', ''),
                'issn': item_data.get('ISSN', ''),
                'call_number': item_data.get('callNumber', ''),
                'access_date': item_data.get('accessDate', ''),
                'rights': item_data.get('rights', ''),
                'extra': item_data.get('extra', ''),
                'collections': '',
                'collections_keys': '',
                'collections_count': 0,
                'notes': '',
                'attachments': '',
                'attachments_count': 0,
                'related_items': '',
                'related_items_count': 0,
                'created_date': item_data.get('dateAdded', ''),
                'modified_date': item_data.get('dateModified', ''),
                'last_modified_by': item_data.get('lastModifiedByUser', ''),
                'version': item.get('version', '')
            }
            
            # è·å–é›†åˆä¿¡æ¯
            collections = item_data.get('collections', [])
            if collections:
                details['collections_count'] = len(collections)
                details['collections_keys'] = '; '.join(collections)
                
                # è·å–é›†åˆåç§°
                all_collections = self._get_all_collections()
                collection_names = []
                for collection_key in collections:
                    collection_name = all_collections.get(collection_key, '')
                    if collection_name:
                        collection_names.append(collection_name)
                details['collections'] = '; '.join(collection_names)
            
            # è·å–é™„ä»¶ä¿¡æ¯
            attachments = item.get('attachments', [])
            if attachments:
                details['attachments_count'] = len(attachments)
                attachment_names = [att.get('data', {}).get('title', '') for att in attachments]
                details['attachments'] = '; '.join(attachment_names)
            
            # è·å–ç›¸å…³é¡¹ç›®ä¿¡æ¯
            related_items = item.get('relatedItems', [])
            if related_items:
                details['related_items_count'] = len(related_items)
                details['related_items'] = '; '.join(related_items)
            
            return details
            
        except Exception as e:
            logger.warning(f"âš ï¸  è·å–é¡¹ç›®è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _extract_authors(self, item_data: Dict[str, Any]) -> str:
        """æå–ä½œè€…ä¿¡æ¯"""
        creators = item_data.get('creators', [])
        if not creators:
            return ''
        
        author_names = []
        for creator in creators:
            if creator.get('creatorType') == 'author':
                name = creator.get('name', '') or f"{creator.get('firstName', '')} {creator.get('lastName', '')}".strip()
                if name:
                    author_names.append(name)
        
        return '; '.join(author_names)
    
    def _extract_abstract(self, item_data: Dict[str, Any]) -> str:
        """æå–æ‘˜è¦ä¿¡æ¯"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ‘˜è¦å­—æ®µ
        abstract = item_data.get('abstractNote', '')
        if not abstract:
            abstract = item_data.get('extra', '')
        if not abstract:
            # ä»notesä¸­æŸ¥æ‰¾æ‘˜è¦
            notes = item_data.get('notes', [])
            for note in notes:
                note_content = note.get('data', {}).get('note', '')
                if 'abstract' in note_content.lower() or 'æ‘˜è¦' in note_content:
                    abstract = note_content
                    break
        
        # é™åˆ¶æ‘˜è¦é•¿åº¦
        abstract_limit = self.abstract_limit
        if len(abstract) > abstract_limit:
            abstract = abstract[:abstract_limit] + '...'
        
        return abstract
    
    def _extract_tags(self, item_data: Dict[str, Any]) -> str:
        """æå–æ ‡ç­¾ä¿¡æ¯"""
        tags = item_data.get('tags', [])
        if not tags:
            return ''
        
        tag_names = [tag.get('tag', '') for tag in tags if tag.get('tag')]
        return '; '.join(tag_names)
    
    def check_missing_items(self, limit: int = None) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        start_time = time.time()
        logger.info("ğŸ” å¼€å§‹æ£€æŸ¥æœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®...")
        
        # è·å–æ‰€æœ‰é¡¹ç›®
        all_items = self._get_all_items(limit)
        self.total_items = len(all_items)
        
        # ç­›é€‰æ ‡å‡†æ–‡çŒ®é¡¹ç›®
        proper_items = [item for item in all_items if self._is_proper_item(item)]
        self.proper_items = len(proper_items)
        
        # ç­›é€‰éœ€è¦åˆ†ç±»çš„é¡¹ç›®ï¼ˆæœªåˆ†ç±»å’Œä¸´æ—¶é›†åˆä¸­çš„æ–‡çŒ®ï¼‰
        unfiled_items = []
        for item in proper_items:
            if self._needs_classification(item):
                unfiled_items.append(item)
        
        self.unfiled_items = len(unfiled_items)
        
        # æ‰¹é‡è·å–è¯¦ç»†ä¿¡æ¯
        detailed_items = self._get_item_details_batch(unfiled_items)
        
        self.exported_items = len(detailed_items)
        
        elapsed_time = time.time() - start_time
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info("ğŸ“Š é¡¹ç›®ç»Ÿè®¡:")
        logger.info(f"   æ€»é¡¹ç›®æ•°: {self.total_items}")
        logger.info(f"   æ ‡å‡†æ–‡çŒ®æ•°: {self.proper_items}")
        logger.info(f"   éœ€è¦åˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®æ•°: {self.unfiled_items}")
        logger.info(f"   å¯¼å‡ºé¡¹ç›®æ•°: {self.exported_items}")
        logger.info(f"   å¤„ç†æ—¶é—´: {elapsed_time:.2f}ç§’")
        
        return detailed_items
    
    def export_items(self, items: List[Dict[str, Any]], output_format: str = 'excel') -> str:
        """å¯¼å‡ºæœªåˆ†ç±»æ–‡çŒ®"""
        if not items:
            logger.warning("âš ï¸  æ²¡æœ‰æœªåˆ†ç±»çš„æ–‡çŒ®éœ€è¦å¯¼å‡º")
            return ""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if output_format.lower() == 'json':
            output_file = f"data/unfiled_proper_items_{timestamp}.json"
        
            export_data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                    'total_items': self.total_items,
                    'proper_items': self.proper_items,
                    'unfiled_items': self.unfiled_items,
                    'exported_items': self.exported_items
            },
            'literature_data': items
        }
        
            try:
                os.makedirs(os.path.dirname(output_file), exist_ok=True)
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
        
                logger.info(f"âœ… æœªåˆ†ç±»æ–‡çŒ®å·²å¯¼å‡ºåˆ°: {output_file}")
                return output_file
    
            except Exception as e:
                logger.error(f"âŒ å¯¼å‡ºJSONæ–‡ä»¶å¤±è´¥: {e}")
                return ""
        
        elif output_format.lower() == 'excel':
            output_file = f"data/unfiled_proper_items_{timestamp}.xlsx"
            
            try:
                df = pd.DataFrame(items)
                os.makedirs(os.path.dirname(output_file), exist_ok=True)
                df.to_excel(output_file, index=False, engine='openpyxl')
                
                logger.info(f"âœ… æœªåˆ†ç±»æ–‡çŒ®å·²å¯¼å‡ºåˆ°: {output_file}")
                return output_file
                
            except Exception as e:
                logger.error(f"âŒ å¯¼å‡ºExcelæ–‡ä»¶å¤±è´¥: {e}")
                return ""
        
        else:
            logger.error(f"âŒ ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="006 - æ£€æŸ¥Zoteroä¸­æœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®ï¼Œå¹¶å¯¼å‡ºä¸ºJSONæˆ–Excelæ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æ£€æŸ¥å¹¶å¯¼å‡ºæœªåˆ†ç±»çš„æ–‡çŒ®ï¼ˆJSONæ ¼å¼ï¼‰
  python 006_check_and_export_missing_proper_items.py --output-format json
  
  # æ£€æŸ¥å¹¶å¯¼å‡ºæœªåˆ†ç±»çš„æ–‡çŒ®ï¼ˆExcelæ ¼å¼ï¼‰
  python 006_check_and_export_missing_proper_items.py --output-format excel
  
  # ä½¿ç”¨schemaæ–‡ä»¶åˆ¤æ–­åˆ†ç±»çŠ¶æ€
  python 006_check_and_export_missing_proper_items.py --schema data/schema_with_collection_keys.json --output-format excel
  
  # é™åˆ¶æ£€æŸ¥æ•°é‡
  python 006_check_and_export_missing_proper_items.py --limit 1000 --output-format json
  
  # è‡ªå®šä¹‰æ‘˜è¦é•¿åº¦é™åˆ¶
  python 006_check_and_export_missing_proper_items.py --abstract-limit 5000 --output-format excel

æ³¨æ„äº‹é¡¹:
  - éœ€è¦é…ç½®Zotero APIç¯å¢ƒå˜é‡
  - åªå¤„ç†æ ‡å‡†çš„Zoteroæ–‡çŒ®ç±»å‹
  - æ’é™¤é™„ä»¶ã€ç¬”è®°ç­‰éæ–‡çŒ®é¡¹ç›®
  - æ”¯æŒJSONå’ŒExcelä¸¤ç§è¾“å‡ºæ ¼å¼
  - ä½¿ç”¨--schemaå‚æ•°æŒ‡å®šåˆ†ç±»schemaæ–‡ä»¶ï¼Œæ ¹æ®schemaä¸­çš„collection_keyåˆ¤æ–­æ–‡çŒ®æ˜¯å¦å·²åˆ†ç±»
  - é»˜è®¤æ‘˜è¦é•¿åº¦é™åˆ¶ä¸º2000å­—ç¬¦ï¼Œå¯ä½¿ç”¨--abstract-limitè‡ªå®šä¹‰
        """
    )
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--limit', type=int, help='é™åˆ¶æ£€æŸ¥çš„æ–‡çŒ®æ•°é‡')
    parser.add_argument('--output-format', type=str, choices=['json', 'excel'], default='excel', 
                       help='è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤: excelï¼‰')
    parser.add_argument('--abstract-limit', type=int, help=f'æ‘˜è¦é•¿åº¦é™åˆ¶ï¼ˆé»˜è®¤: {get_abstract_limit()}å­—ç¬¦ï¼‰')
    parser.add_argument('--schema', type=str, help='è‡ªå®šä¹‰schemaæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè¦†ç›–é»˜è®¤çš„schema_with_collection_keys.json')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = MissingItemsChecker(abstract_limit=args.abstract_limit, schema_file=args.schema)
    
    # æ£€æŸ¥æœªåˆ†ç±»æ–‡çŒ®
    unfiled_items = checker.check_missing_items(limit=args.limit)
    
    if not unfiled_items:
        print("âœ… æ²¡æœ‰å‘ç°æœªåˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®é¡¹ç›®")
        return 0
    
    # å¯¼å‡ºç»“æœ
    output_file = checker.export_items(unfiled_items, args.output_format)
    
    if output_file:
        print(f"\nâœ… æ£€æŸ¥å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   æ€»é¡¹ç›®æ•°: {checker.total_items}")
        print(f"   æ ‡å‡†æ–‡çŒ®æ•°: {checker.proper_items}")
        print(f"   éœ€è¦åˆ†ç±»çš„æ ‡å‡†æ–‡çŒ®æ•°: {checker.unfiled_items}")
        print(f"   å¯¼å‡ºé¡¹ç›®æ•°: {checker.exported_items}")
        print(f"ğŸ“ å¯¼å‡ºæ–‡ä»¶: {output_file}")
        schema_file_for_next_step = args.schema if args.schema else "<è¯·æä¾›ä¸€ä¸ªschemaæ–‡ä»¶ï¼Œä¾‹å¦‚: data/schema_with_collection_keys_YYYYMMDD_HHMMSS.json>"
        print(f"""
ğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:
  1. æ£€æŸ¥å¯¼å‡ºçš„æœªåˆ†ç±»æ–‡çŒ®: {output_file}
  2. ä½¿ç”¨004è„šæœ¬è¿›è¡Œåˆ†ç±»:
     python 004_reclassify_with_new_schema.py --plan --schema {schema_file_for_next_step} --input {output_file}
""")
        return 0
    else:
        print("âŒ å¯¼å‡ºå¤±è´¥")
        return 1


if __name__ == "__main__":
    sys.exit(main()) 