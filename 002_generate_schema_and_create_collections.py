#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
002 - ç”Ÿæˆåˆ†ç±»Schemaå¹¶åˆ›å»ºZoteroé›†åˆ
åŸºäºLLMåˆ†ææ–‡çŒ®ç”Ÿæˆåˆ†ç±»ä½“ç³»ï¼Œåˆ›å»ºZoteroé›†åˆç»“æ„

ä¸»è¦åŠŸèƒ½ï¼š
1. ä½¿ç”¨LLMåˆ†ææ–‡çŒ®ç”Ÿæˆåˆ†ç±»ä½“ç³»
2. ä¿®å¤LLMç”Ÿæˆçš„schemaæ ¼å¼
3. åˆ›å»ºZoteroé›†åˆç»“æ„
4. æ”¯æŒdry-runå’Œæµ‹è¯•æ¨¡å¼

æ³¨æ„ï¼šæ­¤è„šæœ¬ä»…è´Ÿè´£schemaç”Ÿæˆå’Œé›†åˆåˆ›å»ºï¼Œæ–‡çŒ®åˆ†ç±»è¯·ä½¿ç”¨006è„šæœ¬
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, Any, Optional, List
import pandas as pd
import requests
from datetime import datetime

# å¯¼å…¥é…ç½®ç³»ç»Ÿ
from config import (
    get_llm_config, get_zotero_config, get_config,
    get_default_max_items, get_default_test_items, get_default_dry_run_items,
    get_max_tokens_limit, get_default_output_tokens, get_description_preview_length
)
from llm_client import LLMClient

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
# from llm_client import LLMClient

# è®¾ç½®æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from pydantic import BaseModel, ValidationError, Field
import re

class SubCategoryModel(BaseModel):
    name: str
    description: str

class MainCategoryModel(BaseModel):
    name: str
    description: str
    subcategories: dict[str, SubCategoryModel]

class ClassificationSchemaModel(BaseModel):
    main_categories: dict[str, MainCategoryModel]


def verify_schema(schema: dict) -> list[str]:
    """éªŒè¯LLMç”Ÿæˆçš„schemaç»“æ„å’Œå†…å®¹æ˜¯å¦åˆæ³•ï¼Œè¿”å›é”™è¯¯åˆ—è¡¨"""
    errors = []
    try:
        model = ClassificationSchemaModel(**schema)
    except ValidationError as e:
        errors.append(f"ç»“æ„æ ¡éªŒå¤±è´¥: {e}")
        return errors
    
    main_categories = schema.get("main_categories", {})
    if not (5 <= len(main_categories) <= 20):
        errors.append(f"ä¸»åˆ†ç±»æ•°é‡ä¸åœ¨5-20èŒƒå›´: {len(main_categories)}")
    
    for code, main_cat in main_categories.items():
        name = main_cat.get("name", "")
        
        # ä¸»åˆ†ç±»åç§°éªŒè¯
        if not name.startswith("[AUTO]"):
            errors.append(f"ä¸»åˆ†ç±» {code} åç§°æœªä»¥[AUTO]å¼€å¤´: {name}")
        
        # ä¸»åˆ†ç±»è¯æ•°éªŒè¯ï¼ˆç§»é™¤[AUTO]å‰ç¼€åè®¡ç®—ï¼‰
        clean_name = name.replace('[AUTO]', '').strip()
        word_count = len(clean_name.split())
        if not (1 <= word_count <= 10):
            errors.append(f"ä¸»åˆ†ç±» {code} åç§°è¯æ•°ä¸åœ¨1-10: {name}")
        
        # å­åˆ†ç±»éªŒè¯
        subcats = main_cat.get("subcategories", {})
        if not (2 <= len(subcats) <= 10):
            errors.append(f"ä¸»åˆ†ç±» {code} å­åˆ†ç±»æ•°é‡ä¸åœ¨2-10: {len(subcats)}")
        
        for sub_code, sub_cat in subcats.items():
            sub_name = sub_cat.get("name", "")
            
            # å­åˆ†ç±»è¯æ•°éªŒè¯ï¼ˆæ›´å®½æ¾ï¼Œå…è®¸1-10ä¸ªè¯ï¼‰
            sub_word_count = len(sub_name.split())
            if not (1 <= sub_word_count <= 10):
                errors.append(f"å­åˆ†ç±» {sub_code} åç§°è¯æ•°ä¸åœ¨1-10: {sub_name}")
    
    return errors

class SchemaBasedCollectionManager:
    """åŸºäºschemaçš„é›†åˆç®¡ç†å™¨"""
    
    def __init__(self, init_llm: bool = True, init_zotero: bool = True):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        # å¯é€‰åˆå§‹åŒ–LLMå’ŒZoteroå®¢æˆ·ç«¯
        self.llm_client = self._init_llm_client() if init_llm else None
        self.zotero_client = self._init_zotero_client() if init_zotero else None
        
        self.collection_keys = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.collections_created = 0
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            from llm_client import LLMClient
        except ImportError as e:
            logger.error(f"âŒ æ— æ³•å¯¼å…¥LLMClient: {e}")
            return None
        
        # ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿ
        return LLMClient()
    
    def _init_zotero_client(self):
        """åˆå§‹åŒ–Zoteroå®¢æˆ·ç«¯"""
        # ä½¿ç”¨æ–°çš„é…ç½®ç³»ç»Ÿ
        zotero_config = get_zotero_config()
        
        return {
            'user_id': zotero_config.user_id,
            'api_key': zotero_config.api_key,
            'base_url': zotero_config.api_base_url,
            'headers': zotero_config.headers
        }
    
    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬ï¼‰"""
        if not text:
            return 0
        
        # åˆ†ç¦»ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å•è¯
        import re
        
        # ä¸­æ–‡å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', text)
        chinese_token_count = len(chinese_chars)
        
        # è‹±æ–‡å•è¯å’Œå…¶ä»–å­—ç¬¦
        non_chinese_text = re.sub(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]', ' ', text)
        english_words = non_chinese_text.split()
        english_token_count = len(english_words) * 1.3  # è‹±æ–‡å•è¯æŒ‰1.3å€è®¡ç®—
        
        # å…¶ä»–å­—ç¬¦ï¼ˆæ•°å­—ã€æ ‡ç‚¹ç­‰ï¼‰
        other_chars = re.findall(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\s\w]', text)
        other_token_count = len(other_chars) * 0.5  # å…¶ä»–å­—ç¬¦æŒ‰0.5å€è®¡ç®—
        
        total_tokens = chinese_token_count + english_token_count + other_token_count
        return int(total_tokens)
    
    def generate_collections_from_literature(self, literature_file: str, max_items: int = None, dry_run: bool = False, return_schema_only: bool = False) -> Dict[str, str]:
        # ä½¿ç”¨é…ç½®ç³»ç»Ÿè·å–é»˜è®¤å€¼
        if max_items is None:
            max_items = get_default_max_items()
        """ä½¿ç”¨LLMåˆ†ææ–‡çŒ®ç”Ÿæˆåˆç†çš„é›†åˆåˆ†ç±»"""
        if dry_run:
            logger.info("ğŸ” DRY RUNæ¨¡å¼ï¼šå±•ç¤ºLLMç”Ÿæˆé›†åˆçš„è®¡åˆ’...")
        else:
            logger.info("ğŸ§  ä½¿ç”¨LLMåˆ†ææ–‡çŒ®ç”Ÿæˆé›†åˆåˆ†ç±»...")
        
        # åŠ è½½æ–‡çŒ®æ•°æ®
        try:
            df = pd.read_excel(literature_file)
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ–‡çŒ®æ•°æ®: {len(df)} ç¯‡æ–‡çŒ®")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡çŒ®æ•°æ®å¤±è´¥: {e}")
            return {}
        
        # ä½¿ç”¨æ‰€æœ‰æ–‡çŒ®è¿›è¡Œåˆ†æ
        logger.info(f"ğŸ“Š å°†åˆ†ææ‰€æœ‰ {len(df)} ç¯‡æ–‡çŒ®æ¥ç”Ÿæˆåˆ†ç±»")
        
        # è·å–ç°æœ‰é›†åˆï¼ˆä»…åœ¨éæµ‹è¯•æ¨¡å¼ä¸‹ï¼‰
        existing_collections = {}
        if not return_schema_only:
            existing_collections = self._get_existing_collections()
        
        if dry_run:
            print(f"\nğŸ“Š LLMç”Ÿæˆé›†åˆè®¡åˆ’:")
            print(f"åˆ†ææ–‡çŒ®æ•°: {len(df)} ç¯‡")
            print(f"ç°æœ‰é›†åˆ: {len(existing_collections)} ä¸ª")
            print(f"é¢„è®¡LLMè°ƒç”¨: 1æ¬¡ï¼ˆç”Ÿæˆåˆ†ç±»ä½“ç³»ï¼‰")
            print("\nğŸ“‹ åˆ†æå†…å®¹:")
            print(f"- æ–‡çŒ®æ ‡é¢˜å’Œæ‘˜è¦")
            print(f"- ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ")
            print(f"- ä¸»é¢˜èšç±»åˆ†æ")
            print(f"- ç”Ÿæˆå±‚æ¬¡åŒ–åˆ†ç±»ä½“ç³»")
            return {}
        
        # å‡†å¤‡æ‰€æœ‰æ–‡çŒ®æ ·æœ¬ç”¨äºLLMåˆ†æï¼ˆä½¿ç”¨æ›´é•¿çš„æ‘˜è¦ï¼‰
        literature_samples = []
        for idx, row in df.iterrows():
            title = str(row.get('title', '')).strip()
            abstract = str(row.get('abstract', '')).strip()
            
            if title and abstract:
                # ä¿ç•™å®Œæ•´æ‘˜è¦ï¼Œä¸æˆªæ–­ï¼Œè®©LLMè·å¾—æ›´å¤šä¿¡æ¯
                literature_samples.append({
                    'title': title,
                    'abstract': abstract
                })
        
        if not literature_samples:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡çŒ®æ ·æœ¬")
            return {}
        
        # åˆ›å»ºLLMæç¤ºè¯æ¥ç”Ÿæˆåˆ†ç±»ä½“ç³»
        system_prompt = """You are a professional academic literature classification expert specializing in Computer Science and related fields. Your task is to design a comprehensive and well-balanced classification system based on the provided literature samples.\n\nPlease carefully analyze ALL titles and abstracts in the literature samples to identify the complete spectrum of research areas, technical topics, and disciplinary directions. Pay special attention to:\n\n1. **Comprehensive Coverage**: Ensure the classification covers ALL major research areas present in the literature, including but not limited to:\n   - AI/ML (Foundation Models, LLMs, MLLMs, Computer Vision, etc.)\n   - Traditional Systems (Distributed Systems, Operating Systems, Database Systems, etc.)\n   - AI Systems (Training Frameworks, Inference Frameworks, GPU Optimaztions, Attention Optimaztions, etc.)\n   - Scientific Computing (HPC, Physics, Chemistry, Biology applications, etc.)\n   - Graphics and Visualization (3D rendering, 3DGS, NeRF, Computer Graphics, etc.)\n   - Programming Languages and Software Engineering\n   - Infras (Networks, Storage, Security, DataCenters etc.)\n   - Any other CS domains present in the literature\n\n2. **Balanced Representation**: Ensure that all significant research areas in the literature are represented proportionally, without over-emphasizing any single domain.\n\n3. **Hierarchical Structure**: Create a logical two-level hierarchy where:\n   - Main categories represent broad CS research domains\n   - Subcategories represent specific technical directions within each domain\n\n4. **Professional Standards**: Use standard CS terminology and naming conventions that would be recognized by the academic community.\n\nClassification System Requirements:\n1. Main Categories: 8-15 major Computer Science research domains\n2. Subcategories: Each main category must have 4-10 specific technical directions (aim for more granular subcategories)\n3. Category Names: Concise, professional English names using 2-4 words maximum (never exceed 5 words), using standard CS terminology. IMPORTANT: All main category names must be prefixed with "[AUTO]" (e.g., "[AUTO] AI and Machine Learning Models")\n4. Category Descriptions: Clear, accurate descriptions of research scope and content\n5. Coverage: Ensure ALL significant research areas from the literature are covered\n6. Balance: Avoid over-representing any single domain while ensuring comprehensive coverage\n\nPlease return the classification system in JSON format:"""

        # æ„å»ºå®Œæ•´çš„æ–‡çŒ®æ–‡æœ¬ï¼ˆä½¿ç”¨æ‰€æœ‰æ–‡çŒ®ï¼‰
        literature_text = ""
        for i, sample in enumerate(literature_samples):
            literature_text += f"{i+1}. æ ‡é¢˜ï¼š{sample['title']}\n   æ‘˜è¦ï¼š{sample['abstract']}\n\n"

        user_prompt = f"""Please design a comprehensive and well-balanced Computer Science classification system based on the following {len(literature_samples)} literature samples.\n\nLiterature Samples:\n{literature_text}\n\nIMPORTANT: Please ensure that your classification system:\n1. Covers ALL significant research areas present in the literature samples\n2. Provides balanced representation across different CS domains\n3. Includes specific categories for any specialized research areas (e.g., High Energy Physics, Scientific Computing, etc.)\n4. Uses standard Computer Science terminology and naming conventions\n5. Category names should be concise (2-4 words, maximum 5 words)\n6. Create more granular subcategories (4-10 per main category) for better organization\n7. ALL main category names MUST be prefixed with "[AUTO]" (e.g., "[AUTO] AI and Machine Learning Models")\n\nPlease return the classification system in JSON format:\n{{\n    "main_categories": {{\n        "category_code": {{\n            "name": "Category Name",\n            "description": "Category Description",\n            "subcategories": {{\n                "subcategory_code": {{\n                    "name": "Subcategory Name", \n                    "description": "Subcategory Description"\n                }}\n            }}\n        }}\n    }}\n}}\n\nRequirements:\n- All category names and descriptions must be in English\n- Use professional Computer Science terminology\n- Ensure comprehensive coverage of ALL research areas in the literature\n- Maintain clear hierarchical structure with main categories and subcategories\n- Focus on Computer Science domains while including interdisciplinary applications\n- Category names must be concise (2-4 words, never exceed 5 words)\n- Create more granular subcategories (3-6 per main category) for better organization\n- ALL main category names MUST be prefixed with "[AUTO]" (e.g., "[AUTO] AI and Machine Learning Models")"""
        
        # è®¡ç®—tokenä½¿ç”¨é‡
        total_prompt = system_prompt + "\n\n" + user_prompt
        estimated_tokens = self._estimate_tokens(total_prompt)
        # ç®€å•çš„æˆæœ¬ä¼°ç®—ï¼ˆç¾å…ƒï¼‰
        input_cost = (estimated_tokens / 1000) * 0.0035
        default_output_tokens = get_default_output_tokens()
        output_cost = (default_output_tokens / 1000) * 0.105
        estimated_cost = input_cost + output_cost
        
        logger.info(f"ğŸ“Š Tokenä¼°ç®—:")
        logger.info(f"  åˆ†ææ–‡çŒ®æ•°: {len(literature_samples)} ç¯‡")
        logger.info(f"  è¾“å…¥tokens: ~{estimated_tokens:,}")
        logger.info(f"  è¾“å‡ºtokens: ~{default_output_tokens:,}")
        logger.info(f"  æ€»tokens: ~{estimated_tokens + default_output_tokens:,}")
        logger.info(f"  ä¼°ç®—æˆæœ¬: ${estimated_cost:.4f}")
        
        # æ£€æŸ¥tokené™åˆ¶
        max_tokens_limit = get_max_tokens_limit()
        if estimated_tokens > max_tokens_limit:
            logger.warning(f"âš ï¸  ä¼°ç®—tokenæ•°é‡ ({estimated_tokens:,}) è¶…è¿‡{max_tokens_limit:,}é™åˆ¶")
            logger.info("å»ºè®®å‡å°‘æ–‡çŒ®æ•°é‡æˆ–æˆªæ–­æ‘˜è¦")
            return {}
        
        # ç”¨æˆ·ç¡®è®¤ï¼ˆä»…åœ¨édry_runä¸”éreturn_schema_onlyæ—¶ï¼‰
        if not dry_run and not return_schema_only:
            print(f"\nğŸ“Š LLMè¯·æ±‚ä¼°ç®—:")
            print(f"  åˆ†ææ–‡çŒ®æ•°: {len(literature_samples)} ç¯‡")
            print(f"  ä¼°ç®—è¾“å…¥tokens: {estimated_tokens:,}")
            print(f"  ä¼°ç®—è¾“å‡ºtokens: {default_output_tokens:,}")
            print(f"  æ€»tokens: {estimated_tokens + default_output_tokens:,}")
            print(f"  ä¼°ç®—æˆæœ¬: ${estimated_cost:.4f}")
            
            confirm = input("\næ˜¯å¦ç»§ç»­æ‰§è¡Œï¼Ÿ(y/N): ").strip().lower()
            if confirm != 'y':
                logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return {}

        # è°ƒç”¨LLMç”Ÿæˆåˆ†ç±»ä½“ç³»
        try:
            response = self.llm_client.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                max_tokens=default_output_tokens,
                temperature=0.3
            )
            # è§£æLLMå“åº”
            classification_system = self._parse_classification_system(response.get('content', ''))
            if not classification_system:
                logger.error("âŒ LLMç”Ÿæˆçš„åˆ†ç±»ä½“ç³»è§£æå¤±è´¥")
                return {}
            # åœ¨LLMç”Ÿæˆschemaåè°ƒç”¨verify_schema
            errors = verify_schema(classification_system)
            if errors:
                logger.warning("âš ï¸ LLMç”Ÿæˆçš„schemaæœªé€šè¿‡æ ¡éªŒï¼š")
                for err in errors:
                    logger.warning(f"   - {err}")
            else:
                logger.info("âœ… LLMæˆåŠŸç”Ÿæˆåˆ†ç±»ä½“ç³»")
            # å¦‚æœåªéœ€è¦è¿”å›åˆ†ç±»ä½“ç³»ï¼Œä¸åˆ›å»ºé›†åˆ
            if return_schema_only:
                return classification_system
            # åˆ›å»ºé›†åˆ
            return self._create_collections_from_llm_system(classification_system, existing_collections, dry_run)
        except Exception as e:
            logger.error(f"âŒ LLMç”Ÿæˆåˆ†ç±»ä½“ç³»å¤±è´¥: {e}")
            return {}
    
    def _parse_classification_system(self, response: str) -> Dict[str, Any]:
        """è§£æLLMç”Ÿæˆçš„åˆ†ç±»ä½“ç³»"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.error("æ— æ³•æ‰¾åˆ°JSONæ ¼å¼çš„åˆ†ç±»ä½“ç³»")
                return {}
            
            json_str = response[start_idx:end_idx]
            classification_system = json.loads(json_str)
            
            # éªŒè¯ç»“æ„
            if 'main_categories' not in classification_system:
                logger.error("åˆ†ç±»ä½“ç³»ç¼ºå°‘main_categorieså­—æ®µ")
                return {}
            
            return classification_system
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return {}
        except Exception as e:
            logger.error(f"è§£æåˆ†ç±»ä½“ç³»å¤±è´¥: {e}")
            return {}
    
    def save_llm_generated_schema(self, classification_system: Dict[str, Any], output_file: str = None) -> str:
        """ä¿å­˜LLMç”Ÿæˆçš„åŸå§‹schema"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/llm_generated_schema_{timestamp}.json"
        
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(classification_system, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… LLMç”Ÿæˆçš„schemaå·²ä¿å­˜åˆ°: {output_file}")
            return output_file
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜LLMç”Ÿæˆçš„schemaå¤±è´¥: {e}")
            return ""
    
    def save_ready_schema(self, classification_system: Dict[str, Any]) -> str:
        """ä¿å­˜ä¸ºreadyçŠ¶æ€çš„schemaæ–‡ä»¶ï¼ˆç¬¬ä¸€æ­¥è¾“å‡ºï¼‰"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"data/classification_schema_ready_{timestamp}.json"
        excel_output_file = f"data/classification_schema_ready_{timestamp}.xlsx"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        main_categories = classification_system.get('main_categories', {})
        total_main = len(main_categories)
        total_sub = sum(len(cat.get('subcategories', {})) for cat in main_categories.values())
        
        # ç”Ÿæˆé¢„è§ˆä¿¡æ¯
        preview = self._generate_schema_preview(classification_system)
        
        # æ„å»ºreadyçŠ¶æ€çš„schema
        ready_schema = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "status": "ready_for_review",
                "total_main_categories": total_main,
                "total_sub_categories": total_sub,
                "total_categories": total_main + total_sub
            },
            "classification_schema": classification_system,
            "preview": preview
        }
        
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(ready_schema, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Ready schemaå·²ä¿å­˜åˆ°: {output_file}")
            
            # ä¿å­˜Excelæ–‡ä»¶
            self._save_schema_to_excel(classification_system, excel_output_file)
            
            return output_file
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ready schemaå¤±è´¥: {e}")
            return ""

    def _save_schema_to_excel(self, classification_system: Dict[str, Any], excel_output_file: str):
        """å°†ç”Ÿæˆçš„schemaä¿å­˜åˆ°Excelæ–‡ä»¶"""
        try:
            import pandas as pd
            from openpyxl import Workbook
            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side

            excel_data = []
            main_categories = classification_system.get('main_categories', {})

            for main_cat_code, main_cat_info in main_categories.items():
                subcategories = main_cat_info.get('subcategories', {})
                if subcategories:
                    for sub_cat_code, sub_cat_info in subcategories.items():
                        row_data = {
                            'main_category_code': main_cat_code,
                            'main_category_name': main_cat_info.get('name', ''),
                            'main_category_description': main_cat_info.get('description', ''),
                            'subcategory_code': sub_cat_code,
                            'subcategory_name': sub_cat_info.get('name', ''),
                            'subcategory_description': sub_cat_info.get('description', '')
                        }
                        excel_data.append(row_data)
                else:
                    row_data = {
                        'main_category_code': main_cat_code,
                        'main_category_name': main_cat_info.get('name', ''),
                        'main_category_description': main_cat_info.get('description', ''),
                        'subcategory_code': '',
                        'subcategory_name': '',
                        'subcategory_description': ''
                    }
                    excel_data.append(row_data)

            df = pd.DataFrame(excel_data)

            with pd.ExcelWriter(excel_output_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Classification Schema', index=False)
                worksheet = writer.sheets['Classification Schema']

                # è®¾ç½®åˆ—å®½
                column_widths = {
                    'A': 20,
                    'B': 40,
                    'C': 60,
                    'D': 20,
                    'E': 40,
                    'F': 60,
                }
                for col, width in column_widths.items():
                    worksheet.column_dimensions[col].width = width

                # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_alignment = Alignment(horizontal="center", vertical="center")

                for cell in worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = header_alignment

                # è®¾ç½®æ•°æ®è¡Œæ ·å¼
                for row in worksheet.iter_rows(min_row=2):
                    for cell in row:
                        cell.alignment = Alignment(wrap_text=True, vertical="top")

            logger.info(f"âœ… Schemaå·²å¯¼å‡ºåˆ°Excelæ–‡ä»¶: {excel_output_file}")

        except ImportError as e:
            logger.warning(f"âš ï¸ æ— æ³•ç”ŸæˆExcelæ–‡ä»¶ï¼Œç¼ºå°‘ä¾èµ–: {e}")
            logger.info("è¯·å®‰è£…: pip install pandas openpyxl")
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆExcelæ–‡ä»¶å¤±è´¥: {e}")
    
    def _generate_schema_preview(self, classification_system: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆschemaé¢„è§ˆä¿¡æ¯"""
        main_categories = classification_system.get('main_categories', {})
        
        preview = {
            "summary": f"åŸºäºæ–‡çŒ®åˆ†æï¼Œç”Ÿæˆäº†{len(main_categories)}ä¸ªä¸»åˆ†ç±»",
            "main_categories": [],
            "recommendations": [
                "å»ºè®®æ£€æŸ¥åˆ†ç±»åç§°æ˜¯å¦ç¬¦åˆZoteroå‘½åè§„èŒƒ",
                "ç¡®è®¤åˆ†ç±»ç»“æ„æ˜¯å¦åˆç†",
                "æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æˆ–è¿‡äºç»†åˆ†çš„åˆ†ç±»"
            ]
        }
        
        for cat_name, cat_info in main_categories.items():
            sub_cats = cat_info.get('subcategories', {})
            preview["main_categories"].append({
                "name": cat_name,
                "description": cat_info.get('description', ''),
                "sub_categories_count": len(sub_cats),
                "sub_categories": list(sub_cats.keys())
            })
        
        return preview
    
    def get_operation_summary(self, schema_file: str) -> Dict[str, Any]:
        """è·å–æ“ä½œæ‘˜è¦ä¿¡æ¯"""
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_data = json.load(f)
            
            classification_system = schema_data.get('classification_schema', {})
            main_categories = classification_system.get('main_categories', {})
            
            total_main = len(main_categories)
            total_sub = sum(len(cat.get('subcategories', {})) for cat in main_categories.values())
            
            return {
                'main_categories': total_main,
                'sub_categories': total_sub,
                'total_categories': total_main + total_sub
            }
        except Exception as e:
            logger.error(f"âŒ è¯»å–schemaæ–‡ä»¶å¤±è´¥: {e}")
            return {'main_categories': 0, 'sub_categories': 0, 'total_categories': 0}
    
    def create_collections_from_ready_schema(self, schema_file: str, dry_run: bool = False) -> str:
        """ä»ready schemaåˆ›å»ºé›†åˆï¼ˆç¬¬äºŒæ­¥ï¼‰"""
        try:
            # è¯»å–ready schema
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_data = json.load(f)
            
            classification_system = schema_data.get('classification_schema', {})
            if not classification_system:
                logger.error("âŒ Schemaæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°åˆ†ç±»ä½“ç³»")
                return ""
            
            # è·å–ç°æœ‰é›†åˆ
            existing_collections = self._get_existing_collections()
            
            # åˆ›å»ºé›†åˆ
            collection_keys = self._create_collections_from_llm_system(
                classification_system, 
                existing_collections, 
                dry_run=dry_run
            )
            
            if not collection_keys:
                logger.error("âŒ åˆ›å»ºé›†åˆå¤±è´¥")
                return ""
            
            # ä¿å­˜å¸¦collection keysçš„å®Œæ•´schema
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/schema_with_collection_keys_{timestamp}.json"
            
            # æ„å»ºå®Œæ•´schema
            complete_schema = {
            "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "status": "collections_created",
                    "source_file": schema_file,
                    "total_collections_created": len(collection_keys)
                },
                "classification_schema": classification_system,
                "collection_mapping": collection_keys
            }
            try:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(complete_schema, f, ensure_ascii=False, indent=2)
            
                logger.info(f"âœ… å®Œæ•´schemaå·²ä¿å­˜åˆ°: {output_file}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜schemaå¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"âŒ ä»ready schemaåˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return ""
    
    def _create_collections_from_llm_system(self, classification_system: Dict[str, Any], existing_collections: Dict[str, str], dry_run: bool = False) -> Dict[str, str]:
        """ä»LLMç”Ÿæˆçš„åˆ†ç±»ä½“ç³»åˆ›å»ºé›†åˆ"""
        main_categories = classification_system.get('main_categories', {})
        
        if not main_categories:
            logger.error("âŒ åˆ†ç±»ä½“ç³»ä¸­æ²¡æœ‰ä¸»åˆ†ç±»")
            return {}
        
        # è·å–ç°æœ‰é›†åˆçš„åç§°å’Œkeyæ˜ å°„
        existing_names = {name: key for key, name in existing_collections.items()}
        existing_keys = {name: key for key, name in existing_collections.items()}
        
        logger.info(f"ğŸ“Š å¼€å§‹åˆ›å»ºé›†åˆ:")
        logger.info(f"   ä¸»åˆ†ç±»æ•°: {len(main_categories)}")
        logger.info(f"   ç°æœ‰é›†åˆ: {len(existing_collections)}")
        
        # åˆ›å»ºä¸»åˆ†ç±»é›†åˆ
        logger.info(f"ğŸ¯ åˆ›å»ºä¸»åˆ†ç±»é›†åˆ:")
        main_collections = {}
        created_main = 0
        all_collection_keys = {}

        for category_code, category_data in main_categories.items():
            category_name = category_data["name"]
            
            
            
            if dry_run:
                logger.info(f"ğŸ” [å¹²è¿è¡Œ] å°†åˆ›å»ºä¸»åˆ†ç±»: {category_name}")
            else:
                collection_key = self._create_collection(category_name, category_data.get("description", ""))
                if collection_key:
                    main_collections[category_code] = collection_key
                    created_main += 1
                    logger.info(f"âœ… åˆ›å»ºä¸»åˆ†ç±»: {category_name} (key: {collection_key})")
                    existing_names[category_name] = collection_key
                    category_data['collection_key'] = collection_key
                    category_data['collection_key'] = collection_key
                else:
                    logger.error(f"âŒ åˆ›å»ºä¸»åˆ†ç±»å¤±è´¥: {category_name}")
        
        logger.info(f"ğŸ“Š ä¸»åˆ†ç±»åˆ›å»ºå®Œæˆ:")
        logger.info(f"   åˆ›å»ºæˆåŠŸ: {created_main} ä¸ª")
        logger.info(f"   è·³è¿‡å·²å­˜åœ¨: {len(main_categories) - created_main} ä¸ª")
        
        # åˆ›å»ºå­åˆ†ç±»é›†åˆ
        logger.info(f"ğŸ¯ åˆ›å»ºå­åˆ†ç±»é›†åˆ:")
        total_subcategories = 0
        created_subcategories = 0
        
        for category_code, category_data in main_categories.items():
            subcategories = category_data.get("subcategories", {})
            total_subcategories += len(subcategories)
            
            if not subcategories:
                continue
            
            parent_name = category_data["name"]
            logger.info(f"ğŸ“‚ ä¸º {parent_name} åˆ›å»ºå­åˆ†ç±»:")
            
            for sub_cat_code, sub_cat_info in subcategories.items():
                sub_name = sub_cat_info.get("name", "")
                
                
                
                if dry_run:
                    logger.info(f"ğŸ” [å¹²è¿è¡Œ] å°†åˆ›å»ºå­åˆ†ç±»: {sub_name} (çˆ¶åˆ†ç±»: {parent_name})")
                else:
                    parent_key = main_collections.get(category_code)
                    if parent_key:
                        collection_key = self._create_collection(sub_name, parent_key=parent_key)
                        if collection_key:
                            all_collection_keys[sub_cat_code] = collection_key
                            created_subcategories += 1
                            sub_cat_info['collection_key'] = collection_key
                            sub_cat_info['collection_key'] = collection_key
                    else:
                        logger.error(f"âŒ æ— æ³•åˆ›å»ºå­åˆ†ç±» {sub_name} - çˆ¶åˆ†ç±» {parent_name} ä¸å­˜åœ¨")
        
        logger.info(f"ğŸ“Š åˆ›å»ºå®Œæˆç»Ÿè®¡:")
        logger.info(f"   ä¸»åˆ†ç±»æ€»æ•°: {len(main_categories)}")
        logger.info(f"   å­åˆ†ç±»æ€»æ•°: {total_subcategories}")
        logger.info(f"   å®é™…åˆ›å»ºå­åˆ†ç±»: {created_subcategories}")
        
        # ä¿å­˜æ›´æ–°åçš„schemaï¼ˆåŒ…å«collection_keyï¼‰
        if not dry_run:
            # åˆ›å»ºå¸¦collection_keyçš„schemaæ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            updated_schema_file = f"data/schema_with_collection_keys_{timestamp}.json"
            
            # æ„å»ºå®Œæ•´çš„schema
            complete_schema = {
                "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "status": "collections_created",
                    "total_collections_created": len(main_collections) + created_subcategories
                },
                "classification_schema": classification_system,
                "collection_mapping": main_collections
            }
            
            with open(updated_schema_file, 'w', encoding='utf-8') as f:
                json.dump(complete_schema, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ‰€æœ‰é›†åˆåˆ›å»ºå®Œæˆï¼")
            logger.info(f"ğŸ“ æ›´æ–°åçš„schemaå·²ä¿å­˜åˆ°: {updated_schema_file}")
            return updated_schema_file
        else:
            logger.info(f"ğŸ” å¹²è¿è¡Œå®Œæˆï¼Œæœªå®é™…åˆ›å»ºä»»ä½•é›†åˆ")
            return None
    
    def _get_existing_collections(self) -> Dict[str, str]:
        """è·å–ç°æœ‰é›†åˆ"""
        try:
            # ä½¿ç”¨é…ç½®ç³»ç»Ÿ
            zotero_config = get_zotero_config()
            
            url = f"https://api.zotero.org/users/{zotero_config.user_id}/collections"
            headers = zotero_config.headers
            
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            collections = response.json()
            collection_dict = {}
            
            for collection in collections:
                key = collection.get('key')
                name = collection.get('data', {}).get('name', '')
                if key and name:
                    collection_dict[key] = name
            
            logger.info(f"âœ… è·å–åˆ° {len(collection_dict)} ä¸ªç°æœ‰é›†åˆ")
            return collection_dict
            
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆå¼‚å¸¸: {e}")
            return {}
    
    def _create_collection(self, name: str, description: str = "", parent_key: str = None) -> str:
        """åˆ›å»ºé›†åˆ"""
        try:
            # ä½¿ç”¨é…ç½®ç³»ç»Ÿ
            zotero_config = get_zotero_config()
            
            url = f"https://api.zotero.org/users/{zotero_config.user_id}/collections"
            headers = zotero_config.headers
            
            # æ„å»ºé›†åˆæ•°æ®
            collection_data = [{
                "name": name
            }]
            
            # å¦‚æœæœ‰çˆ¶é›†åˆï¼Œæ·»åŠ åˆ°çˆ¶é›†åˆä¸‹
            if parent_key:
                collection_data[0]["parentCollection"] = parent_key
            
            response = requests.post(url, headers=headers, json=collection_data)
            response.raise_for_status()
            
            result = response.json()
            if result.get('successful') and '0' in result['successful']:
                collection_key = result['successful']['0']['key']
            else:
                logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {result}")
                collection_key = None
            
            if collection_key:
                logger.info(f"âœ… åˆ›å»ºé›†åˆæˆåŠŸ: {name} (key: {collection_key})")
                return collection_key
            else:
                logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: æœªè·å–åˆ°key")
                return ""
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            return ""
    
    def save_collection_mapping(self, output_file: str = None) -> str:
        """ä¿å­˜é›†åˆæ˜ å°„"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"data/collection_mapping_{timestamp}.json"
        
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.collection_keys, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… é›†åˆæ˜ å°„å·²ä¿å­˜åˆ°: {output_file}")
            return output_file
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é›†åˆæ˜ å°„å¤±è´¥: {e}")
            return ""


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="002 - åŸºäºæ–‡çŒ®ä¿¡æ¯ç”Ÿæˆåˆ†ç±»Schemaï¼Œå¹¶å¯é€‰åœ°åœ¨Zoteroä¸­åˆ›å»ºå¯¹åº”çš„é›†åˆç»“æ„",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆåˆ†ç±»ä½“ç³»æ–‡ä»¶ï¼ˆå®‰å…¨æ“ä½œï¼‰
  python 002_generate_schema_and_create_collections.py --generate-schema --input data/literature_info.xlsx --max-items 100
  
  # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºZoteroé›†åˆï¼ˆå±é™©æ“ä½œï¼‰
  python 002_generate_schema_and_create_collections.py --create-collections --schema data/classification_schema_ready.json
  
  # ä½¿ç”¨å¹²è¿è¡Œæ¨¡å¼é¢„è§ˆç¬¬äºŒæ­¥
  python 002_generate_schema_and_create_collections.py --create-collections --schema data/classification_schema_ready.json --dry-run

  # æµ‹è¯•æ¨¡å¼
  python 002_generate_schema_and_create_collections.py --test --input data/literature_info.xlsx

æ³¨æ„äº‹é¡¹:
  - éœ€è¦é…ç½®LLM APIç¯å¢ƒå˜é‡
  - éœ€è¦é…ç½®Zotero APIç¯å¢ƒå˜é‡
  - å»ºè®®å…ˆä½¿ç”¨--testæˆ–--dry-runæ¨¡å¼æµ‹è¯•
  - åˆ›å»ºé›†åˆæ“ä½œä¼šæ°¸ä¹…ä¿®æ”¹Zoteroåº“ï¼Œè¯·è°¨æ…æ“ä½œ
        """
    )
    
    # åˆ›å»ºäº’æ–¥ç»„
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆä½¿ç”¨è¾ƒå°‘æ•°æ®ï¼‰')
    mode_group.add_argument('--generate-schema', action='store_true', help='ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆåˆ†ç±»ä½“ç³»æ–‡ä»¶ï¼ˆå®‰å…¨æ“ä½œï¼‰')
    mode_group.add_argument('--create-collections', action='store_true', help='ç¬¬äºŒæ­¥ï¼šåˆ›å»ºZoteroé›†åˆï¼ˆå±é™©æ“ä½œï¼‰')
    
    # æ–‡ä»¶è·¯å¾„å‚æ•°
    parser.add_argument('--input', type=str, help='æ–‡çŒ®æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆExcelæ ¼å¼ï¼‰')
    parser.add_argument('--schema', type=str, help='åˆ†ç±»schemaæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--max-items', type=int, help='æœ€å¤§å¤„ç†æ–‡çŒ®æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨æ‰€æœ‰æ–‡çŒ®ï¼‰')
    parser.add_argument('--dry-run', action='store_true', help='å¹²è¿è¡Œæ¨¡å¼ï¼Œåªæ˜¾ç¤ºè®¡åˆ’ï¼Œä¸å®é™…åˆ›å»º')
    
    args = parser.parse_args()
    
    # æ ¹æ®æ¨¡å¼éªŒè¯å¿…éœ€å‚æ•°
    if args.test or args.generate_schema:
        if not args.input:
            parser.error("--test å’Œ --generate-schema æ¨¡å¼éœ€è¦æŒ‡å®š --input å‚æ•°")
        if not os.path.exists(args.input):
            parser.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
    
    if args.create_collections:
        if not args.schema:
            parser.error("--create-collections æ¨¡å¼éœ€è¦æŒ‡å®š --schema å‚æ•°")
        if not os.path.exists(args.schema):
            parser.error(f"Schemaæ–‡ä»¶ä¸å­˜åœ¨: {args.schema}")
    
    # åˆ›å»ºç®¡ç†å™¨ï¼ˆæ ¹æ®æ¨¡å¼å†³å®šåˆå§‹åŒ–ï¼‰
    if args.create_collections:
        # åˆ›å»ºé›†åˆæ¨¡å¼éœ€è¦Zoteroå®¢æˆ·ç«¯
        manager = SchemaBasedCollectionManager(init_llm=False, init_zotero=True)
    else:
        # å…¶ä»–æ¨¡å¼éœ€è¦LLMå®¢æˆ·ç«¯
        manager = SchemaBasedCollectionManager(init_llm=True, init_zotero=False)
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.test:
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {args.max_items or get_default_test_items()} ç¯‡æ–‡çŒ®ç”Ÿæˆåˆ†ç±»ä½“ç³»...")
        
        classification_system = manager.generate_collections_from_literature(
            literature_file=args.input, 
            max_items=args.max_items or get_default_test_items(), 
            dry_run=False,
            return_schema_only=True
        )
        
        if classification_system:
            schema_file = manager.save_llm_generated_schema(classification_system)
            if schema_file:
                print(f"\nâœ… æµ‹è¯•å®Œæˆï¼åˆ†ç±»ä½“ç³»å·²ä¿å­˜åˆ°: {schema_file}")
                return 0
            else:
                print("âŒ ä¿å­˜åˆ†ç±»ä½“ç³»å¤±è´¥")
                return 1
        else:
            print("âŒ ç”Ÿæˆåˆ†ç±»ä½“ç³»å¤±è´¥")
            return 1
            
    elif args.generate_schema:
        print(f"\nğŸ“ ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆåˆ†ç±»ä½“ç³»æ–‡ä»¶ï¼ˆå®‰å…¨æ“ä½œï¼‰...")
        
        # ç”Ÿæˆå¹¶ä¿®å¤schema
        classification_system = manager.generate_collections_from_literature(
            literature_file=args.input, 
            max_items=args.max_items, 
            dry_run=False,
            return_schema_only=True
        )
        
        if classification_system:
            # ä¿å­˜ä¸ºreadyçŠ¶æ€çš„æ–‡ä»¶
            ready_schema_file = manager.save_ready_schema(classification_system)
            if ready_schema_file:
                excel_file = ready_schema_file.replace('.json', '.xlsx')
                print(f"\nâœ… ç¬¬ä¸€æ­¥å®Œæˆï¼åˆ†ç±»ä½“ç³»å·²ä¿å­˜ã€‚")
                print(f"   - JSON: {ready_schema_file}")
                print(f"   - Excel: {excel_file}")
                print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
                print(f"  1. æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶: {excel_file} å’Œ {ready_schema_file}")
                print(f"  2. ç¡®è®¤åˆ†ç±»ä½“ç³»åˆç†åï¼Œè¿è¡Œ:")
                print(f"     python 003_convert_schema_format.py --new-to-old --input {ready_schema_file}")
                return 0
            else:
                print("âŒ ä¿å­˜åˆ†ç±»ä½“ç³»å¤±è´¥")
                return 1
        else:
            print("âŒ ç”Ÿæˆåˆ†ç±»ä½“ç³»å¤±è´¥")
            return 1
        
    elif args.create_collections:
        print(f"\nğŸ—ï¸  ç¬¬äºŒæ­¥ï¼šåˆ›å»ºZoteroé›†åˆ...")
        
        # æ˜¾ç¤ºæ“ä½œæ‘˜è¦
        summary = manager.get_operation_summary(args.schema)
        print(f"\nğŸ“Š æ“ä½œæ‘˜è¦:")
        print(f"  ä¸»åˆ†ç±»æ•°: {summary['main_categories']}")
        print(f"  å­åˆ†ç±»æ•°: {summary['sub_categories']}")
        print(f"  æ€»åˆ†ç±»æ•°: {summary['total_categories']}")
        
        if not args.dry_run:
            confirm = input(f"\nâš ï¸  è¿™æ˜¯ä¸€ä¸ªå±é™©æ“ä½œï¼Œå°†æ°¸ä¹…ä¿®æ”¹æ‚¨çš„Zoteroåº“ã€‚ç¡®è®¤è¦åˆ›å»ºè¿™äº›é›†åˆå—ï¼Ÿ(y/N): ").strip().lower()
            if confirm != 'y':
                print("æ“ä½œå·²å–æ¶ˆ")
                return 0
    
        # åˆ›å»ºé›†åˆ
        result_file = manager.create_collections_from_ready_schema(args.schema, args.dry_run)
        if result_file or args.dry_run:
            if args.dry_run:
                print("\nâœ… å¹²è¿è¡Œå®Œæˆï¼Œæœªå¯¹Zoteroè¿›è¡Œä»»ä½•ä¿®æ”¹ã€‚")
            else:
                print(f"âœ… ç¬¬äºŒæ­¥å®Œæˆï¼é›†åˆå·²åˆ›å»ºï¼Œå®Œæ•´schemaå·²ä¿å­˜åˆ°: {result_file}")
                print("ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
                print(f"  1. æ£€æŸ¥Zoteroä¸­çš„é›†åˆç»“æ„")
                print(f"  2. è¿è¡Œ: python 004_reclassify_with_new_schema.py --plan --schema {result_file}")
            return 0
        else:
            print("âŒ åˆ›å»ºé›†åˆå¤±è´¥")
            return 1


if __name__ == "__main__":
    sys.exit(main())