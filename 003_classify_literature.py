#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
003 - Classify Literature
ä½¿ç”¨LLMå¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±»ï¼Œæ”¯æŒå¤šè¿›ç¨‹å¹¶å‘
"""

import os
import sys
import json
import pandas as pd
import multiprocessing as mp
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import glob
import time

# å¯¼å…¥å·²æœ‰çš„æ¨¡å—
from llm_client import LLMClient

# å…¨å±€LLMå®¢æˆ·ç«¯é…ç½®ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
OPENAI_API_KEY = None
OPENAI_BASE_URL = None

def init_worker():
    """å·¥ä½œè¿›ç¨‹åˆå§‹åŒ–å‡½æ•°"""
    global OPENAI_API_KEY, OPENAI_BASE_URL
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')

def classify_single_literature(args):
    """åˆ†ç±»å•ç¯‡æ–‡çŒ®çš„å·¥ä½œå‡½æ•°ï¼ˆä¸¤é˜¶æ®µåˆ†ç±»ï¼šä¸»åˆ†ç±»->å­åˆ†ç±»ï¼‰"""
    literature_info, classification_schema, worker_id, classifier_instance = args
    
    try:
        # åœ¨å·¥ä½œè¿›ç¨‹ä¸­åˆ›å»ºLLMå®¢æˆ·ç«¯
        llm = LLMClient(
            model_name="gpt-4.1",
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        # æ„å»ºæ‰€æœ‰åˆ†ç±»çš„keyæ˜ å°„
        all_collections_map = build_collections_key_map(classification_schema)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šé€‰æ‹©ä¸»åˆ†ç±»
        main_categories_prompt = build_main_classification_prompt(literature_info, classification_schema)
        
        main_response = llm.generate(
            prompt=main_categories_prompt,
            temperature=0.3,
            max_tokens=800
        )
        
        main_content = main_response.get('content', '').strip()
        
        try:
            main_result = json.loads(main_content)
            selected_main_keys = main_result.get('selected_main_categories', [])
            
            # éªŒè¯ä¸»åˆ†ç±»keyçš„æœ‰æ•ˆæ€§
            valid_main_keys = validate_collection_keys(selected_main_keys, classification_schema, 'main', 
                                                      classifier=classifier_instance)
            
            if not valid_main_keys:
                return {
                    'success': False,
                    'item_key': literature_info['item_key'],
                    'title': literature_info['title'],
                    'worker_id': worker_id,
                    'error': 'æœªé€‰æ‹©æœ‰æ•ˆçš„ä¸»åˆ†ç±»',
                    'main_response': main_content
                }
            
        except json.JSONDecodeError:
            return {
                'success': False,
                'item_key': literature_info['item_key'],
                'title': literature_info['title'],
                'worker_id': worker_id,
                'error': 'ä¸»åˆ†ç±»JSONè§£æå¤±è´¥',
                'main_response': main_content
            }
        
        # ç¬¬äºŒé˜¶æ®µï¼šä¸ºæ¯ä¸ªé€‰å®šçš„ä¸»åˆ†ç±»é€‰æ‹©å­åˆ†ç±»
        final_collection_keys = []
        sub_responses = []
        
        for main_key in valid_main_keys:
            # æ„å»ºå­åˆ†ç±»é€‰æ‹©æç¤ºè¯
            sub_prompt = build_sub_classification_prompt(literature_info, classification_schema, main_key)
            
            sub_response = llm.generate(
                prompt=sub_prompt,
                temperature=0.3,
                max_tokens=600
            )
            
            sub_content = sub_response.get('content', '').strip()
            sub_responses.append(sub_content)
            
            try:
                sub_result = json.loads(sub_content)
                selected_sub_keys = sub_result.get('selected_subcategories', [])
                
                # éªŒè¯å­åˆ†ç±»keyçš„æœ‰æ•ˆæ€§
                valid_sub_keys = validate_collection_keys(selected_sub_keys, classification_schema, 'sub', 
                                                         main_key, classifier=classifier_instance)
                
                # å¦‚æœæ²¡æœ‰åˆé€‚çš„å­åˆ†ç±»ï¼Œå°±ä½¿ç”¨ä¸»åˆ†ç±»æœ¬èº«
                if valid_sub_keys:
                    final_collection_keys.extend(valid_sub_keys)
                else:
                    final_collection_keys.append(main_key)
                    
            except json.JSONDecodeError:
                # å¦‚æœå­åˆ†ç±»è§£æå¤±è´¥ï¼Œä½¿ç”¨ä¸»åˆ†ç±»
                final_collection_keys.append(main_key)
        
        # å»é‡
        final_collection_keys = list(set(final_collection_keys))
        
        # å°†keyè½¬æ¢ä¸ºåç§°ç”¨äºæ˜¾ç¤º
        collection_names = [all_collections_map.get(key, key) for key in final_collection_keys]
        
        return {
            'success': True,
            'item_key': literature_info['item_key'],
            'title': literature_info['title'],
            'worker_id': worker_id,
            'recommended_collection_keys': final_collection_keys,
            'recommended_collections': collection_names,
            'main_analysis': main_result.get('analysis', ''),
            'main_response': main_content,
            'sub_responses': sub_responses
        }
        
    except Exception as e:
        return {
            'success': False,
            'item_key': literature_info['item_key'],
            'title': literature_info['title'],
            'worker_id': worker_id,
            'error': str(e),
            'main_response': '',
            'sub_responses': []
        }

def build_collections_key_map(classification_schema: Dict[str, Any]) -> Dict[str, str]:
    """æ„å»ºåˆ†ç±»keyåˆ°åç§°çš„æ˜ å°„"""
    key_map = {}
    
    # ä¸»åˆ†ç±»
    main_categories = classification_schema.get('main_categories', {})
    for main_cat_name, main_cat_info in main_categories.items():
        key = main_cat_info.get('collection_key', '')
        if key:
            key_map[key] = main_cat_name
        
        # å­åˆ†ç±»
        for sub_cat in main_cat_info.get('subcategories', []):
            sub_key = sub_cat.get('collection_key', '')
            if sub_key:
                key_map[sub_key] = sub_cat['name']
    
    # ç‹¬ç«‹åˆ†ç±»
    independent_categories = classification_schema.get('independent_categories', {})
    for indep_cat_name, indep_cat_info in independent_categories.items():
        key = indep_cat_info.get('collection_key', '')
        if key:
            key_map[key] = indep_cat_name
    
    return key_map

def validate_collection_keys(keys: List[str], classification_schema: Dict[str, Any], 
                           category_type: str, main_key: str = None, classifier=None) -> List[str]:
    """éªŒè¯åˆ†ç±»keyçš„æœ‰æ•ˆæ€§ï¼Œæ”¯æŒåç§°åˆ°keyçš„è‡ªåŠ¨è½¬æ¢"""
    
    # å¦‚æœæä¾›äº†classifierå®ä¾‹ï¼Œå°è¯•å°†åç§°è½¬æ¢ä¸ºkeys
    converted_keys = keys
    if classifier and hasattr(classifier, 'convert_names_to_keys'):
        converted_keys = classifier.convert_names_to_keys(keys)
    
    valid_keys = []
    
    if category_type == 'main':
        # éªŒè¯ä¸»åˆ†ç±»key
        main_categories = classification_schema.get('main_categories', {})
        independent_categories = classification_schema.get('independent_categories', {})
        
        valid_main_keys = set()
        for main_cat_info in main_categories.values():
            if main_cat_info.get('collection_key'):
                valid_main_keys.add(main_cat_info['collection_key'])
        
        for indep_cat_info in independent_categories.values():
            if indep_cat_info.get('collection_key'):
                valid_main_keys.add(indep_cat_info['collection_key'])
        
        for key in converted_keys:
            if key in valid_main_keys:
                valid_keys.append(key)
    
    elif category_type == 'sub' and main_key:
        # éªŒè¯å­åˆ†ç±»key
        main_categories = classification_schema.get('main_categories', {})
        
        valid_sub_keys = set()
        for main_cat_info in main_categories.values():
            if main_cat_info.get('collection_key') == main_key:
                for sub_cat in main_cat_info.get('subcategories', []):
                    if sub_cat.get('collection_key'):
                        valid_sub_keys.add(sub_cat['collection_key'])
                break
        
        for key in converted_keys:
            if key in valid_sub_keys:
                valid_keys.append(key)
    
    return valid_keys

def build_main_classification_prompt(literature_info: Dict[str, Any], classification_schema: Dict[str, Any]) -> str:
    """æ„å»ºä¸»åˆ†ç±»é€‰æ‹©æç¤ºè¯"""
    
    # æ„å»ºæ–‡çŒ®ä¿¡æ¯
    literature_text = build_literature_text(literature_info)
    
    # æ„å»ºä¸»åˆ†ç±»åˆ—è¡¨
    main_categories_text = build_main_categories_text(classification_schema)
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®åˆ†ç±»ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ–‡çŒ®é€‰æ‹©åˆé€‚çš„ä¸»åˆ†ç±»ã€‚

**ä»»åŠ¡è¯´æ˜ï¼š**
è¿™æ˜¯åˆ†ç±»çš„ç¬¬ä¸€é˜¶æ®µï¼šä»æ‰€æœ‰ä¸»åˆ†ç±»ä¸­é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»ã€‚ä½ å¯ä»¥é€‰æ‹©å¤šä¸ªä¸»åˆ†ç±»ã€‚

**æ–‡çŒ®ä¿¡æ¯ï¼š**
{literature_text}

**å¯é€‰çš„ä¸»åˆ†ç±»ï¼š**
{main_categories_text}

**åˆ†ç±»åŸåˆ™ï¼š**
1. **ç²¾ç¡®åŒ¹é…**ï¼šä»”ç»†åˆ†ææ–‡çŒ®å†…å®¹ï¼Œé€‰æ‹©æœ€åŒ¹é…çš„ä¸»åˆ†ç±»
2. **æ”¯æŒå¤šé€‰**ï¼šä¸€ç¯‡æ–‡çŒ®å¯ä»¥åŒæ—¶å±äºå¤šä¸ªä¸»åˆ†ç±»
3. **å¿…é¡»é€‰æ‹©**ï¼šå¿…é¡»è‡³å°‘é€‰æ‹©ä¸€ä¸ªä¸»åˆ†ç±»
4. **åªè¿”å›KEY**ï¼šåªè¿”å›[KEY: ]ä¸­çš„collection_keyï¼Œä¸è¦è¿”å›åˆ†ç±»åç§°ï¼

**å›å¤æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š**
{{
    "selected_main_categories": [
        "EXACT_COLLECTION_KEY1",
        "EXACT_COLLECTION_KEY2"
    ],
    "analysis": "åˆ†æè¯´æ˜ï¼šä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›ä¸»åˆ†ç±»ï¼Œè¯·è¯´æ˜æ–‡çŒ®å†…å®¹ä¸æ‰€é€‰ä¸»åˆ†ç±»çš„åŒ¹é…ç‚¹"
}}

âš ï¸ é‡è¦ï¼šåªè¿”å›[KEY: ]ä¸­æ˜¾ç¤ºçš„ç¡®åˆ‡collection_keyï¼Œä¸è¦è¿”å›åˆ†ç±»åç§°æˆ–å…¶ä»–æ–‡å­—ï¼"""

    return prompt

def build_sub_classification_prompt(literature_info: Dict[str, Any], classification_schema: Dict[str, Any], main_key: str) -> str:
    """æ„å»ºå­åˆ†ç±»é€‰æ‹©æç¤ºè¯"""
    
    # è·å–ä¸»åˆ†ç±»ä¿¡æ¯
    main_cat_name, main_cat_info = get_main_category_by_key(classification_schema, main_key)
    
    # æ„å»ºå­åˆ†ç±»åˆ—è¡¨
    sub_categories_text = build_sub_categories_text(main_cat_info)
    
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯æ–‡çŒ®åˆ†ç±»ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹æ–‡çŒ®åœ¨æŒ‡å®šä¸»åˆ†ç±»ä¸‹é€‰æ‹©åˆé€‚çš„å­åˆ†ç±»ã€‚

**ä»»åŠ¡è¯´æ˜ï¼š**
è¿™æ˜¯åˆ†ç±»çš„ç¬¬äºŒé˜¶æ®µï¼šåœ¨ä¸»åˆ†ç±»"{main_cat_name}"ä¸‹é€‰æ‹©æœ€åˆé€‚çš„å­åˆ†ç±»ã€‚ä½ å¯ä»¥é€‰æ‹©å¤šä¸ªå­åˆ†ç±»ï¼Œä¹Ÿå¯ä»¥ä¸é€‰æ‹©ä»»ä½•å­åˆ†ç±»ï¼ˆå¦‚æœéƒ½ä¸åˆé€‚ï¼‰ã€‚

**æ–‡çŒ®ä¿¡æ¯ï¼š**
{build_literature_text(literature_info)}

**ä¸»åˆ†ç±»ï¼š{main_cat_name}**
æè¿°ï¼š{main_cat_info.get('description', '')}

**å¯é€‰çš„å­åˆ†ç±»ï¼š**
{sub_categories_text}

**åˆ†ç±»åŸåˆ™ï¼š**
1. **ç²¾ç¡®åŒ¹é…**ï¼šé€‰æ‹©ä¸æ–‡çŒ®å†…å®¹æœ€åŒ¹é…çš„å­åˆ†ç±»
2. **æ”¯æŒå¤šé€‰**ï¼šå¯ä»¥é€‰æ‹©å¤šä¸ªç›¸å…³çš„å­åˆ†ç±»
3. **å¯ä»¥ä¸é€‰**ï¼šå¦‚æœæ²¡æœ‰åˆé€‚çš„å­åˆ†ç±»ï¼Œè¿”å›ç©ºæ•°ç»„
4. **åªè¿”å›KEY**ï¼šåªè¿”å›[KEY: ]ä¸­çš„collection_keyï¼Œä¸è¦è¿”å›åˆ†ç±»åç§°ï¼

**å›å¤æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š**
{{
    "selected_subcategories": [
        "EXACT_SUBCATEGORY_KEY1",
        "EXACT_SUBCATEGORY_KEY2"
    ],
    "analysis": "åˆ†æè¯´æ˜ï¼šä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›å­åˆ†ç±»ï¼Œæˆ–ä¸ºä»€ä¹ˆä¸é€‰æ‹©ä»»ä½•å­åˆ†ç±»"
}}

âš ï¸ é‡è¦ï¼šåªè¿”å›[KEY: ]ä¸­æ˜¾ç¤ºçš„ç¡®åˆ‡collection_keyï¼Œä¸è¦è¿”å›åˆ†ç±»åç§°æˆ–å…¶ä»–æ–‡å­—ï¼"""

    return prompt

def build_main_categories_text(classification_schema: Dict[str, Any]) -> str:
    """æ„å»ºä¸»åˆ†ç±»åˆ—è¡¨æ–‡æœ¬"""
    categories_lines = []
    
    # ä¸»åˆ†ç±»
    main_categories = classification_schema.get('main_categories', {})
    for main_cat_name, main_cat_info in main_categories.items():
        key = main_cat_info.get('collection_key', '')
        description = main_cat_info.get('description', '')
        categories_lines.append(f"- **[KEY: {key}]** {main_cat_name}")
        categories_lines.append(f"  æè¿°: {description}")
        categories_lines.append("")
    
    # ç‹¬ç«‹åˆ†ç±»ä¹Ÿä½œä¸ºä¸»åˆ†ç±»
    independent_categories = classification_schema.get('independent_categories', {})
    for indep_cat_name, indep_cat_info in independent_categories.items():
        key = indep_cat_info.get('collection_key', '')
        description = indep_cat_info.get('description', '')
        categories_lines.append(f"- **[KEY: {key}]** {indep_cat_name}")
        categories_lines.append(f"  æè¿°: {description}")
        categories_lines.append("")
    
    return "\n".join(categories_lines)

def build_sub_categories_text(main_cat_info: Dict[str, Any]) -> str:
    """æ„å»ºå­åˆ†ç±»åˆ—è¡¨æ–‡æœ¬"""
    subcategories = main_cat_info.get('subcategories', [])
    
    if not subcategories:
        return "è¯¥ä¸»åˆ†ç±»ä¸‹æ²¡æœ‰å­åˆ†ç±»ã€‚"
    
    categories_lines = []
    for sub_cat in subcategories:
        name = sub_cat.get('name', '')
        key = sub_cat.get('collection_key', '')
        description = sub_cat.get('description', '')
        categories_lines.append(f"- **[KEY: {key}]** {name}")
        categories_lines.append(f"  æè¿°: {description}")
        categories_lines.append("")
    
    return "\n".join(categories_lines)

def get_main_category_by_key(classification_schema: Dict[str, Any], main_key: str) -> tuple:
    """æ ¹æ®keyè·å–ä¸»åˆ†ç±»ä¿¡æ¯"""
    # åœ¨ä¸»åˆ†ç±»ä¸­æŸ¥æ‰¾
    main_categories = classification_schema.get('main_categories', {})
    for main_cat_name, main_cat_info in main_categories.items():
        if main_cat_info.get('collection_key') == main_key:
            return main_cat_name, main_cat_info
    
    # åœ¨ç‹¬ç«‹åˆ†ç±»ä¸­æŸ¥æ‰¾
    independent_categories = classification_schema.get('independent_categories', {})
    for indep_cat_name, indep_cat_info in independent_categories.items():
        if indep_cat_info.get('collection_key') == main_key:
            return indep_cat_name, indep_cat_info
    
    return f"Unknown({main_key})", {}

def build_literature_text(literature_info: Dict[str, Any]) -> str:
    """æ„å»ºæ–‡çŒ®ä¿¡æ¯æ–‡æœ¬"""
    info_lines = []
    
    # åŸºæœ¬ä¿¡æ¯
    info_lines.append(f"ğŸ“„ æ ‡é¢˜: {literature_info.get('title', 'æ— æ ‡é¢˜')}")
    info_lines.append(f"ğŸ“‹ ç±»å‹: {literature_info.get('item_type', 'unknown')}")
    
    # ä½œè€…
    authors = literature_info.get('authors', '')
    if authors:
        info_lines.append(f"ğŸ‘¤ ä½œè€…: {authors}")
    
    # å‘è¡¨ä¿¡æ¯
    pub_title = literature_info.get('publication_title', '')
    conf_name = literature_info.get('conference_name', '')
    if pub_title:
        info_lines.append(f"ğŸ“– æœŸåˆŠ: {pub_title}")
    if conf_name:
        info_lines.append(f"ğŸ›ï¸ ä¼šè®®: {conf_name}")
    
    # æ—¶é—´
    date = literature_info.get('date', '')
    if date:
        info_lines.append(f"ğŸ“… æ—¶é—´: {date}")
    
    # DOI
    doi = literature_info.get('doi', '')
    if doi:
        info_lines.append(f"ğŸ”— DOI: {doi}")
    
    # æ‘˜è¦
    abstract = literature_info.get('abstract', '')
    if abstract:
        if hasattr(abstract, '__len__'):
            abstract_preview = abstract[:500] + '...' if len(abstract) > 500 else abstract
        else:
            abstract_preview = str(abstract)
        info_lines.append(f"ğŸ“ æ‘˜è¦: {abstract_preview}")
    
    # æ ‡ç­¾
    tags = literature_info.get('tags', '')
    if tags:
        info_lines.append(f"ğŸ·ï¸ æ ‡ç­¾: {tags}")
    
    # å½“å‰åˆ†ç±»çŠ¶æ€
    collections_count = literature_info.get('collections_count', 0)
    if collections_count > 0:
        info_lines.append(f"ğŸ“‚ å½“å‰åˆ†ç±»æ•°: {collections_count}")
    else:
        info_lines.append("ğŸ“‚ å½“å‰åˆ†ç±»: æ— ")
    
    return "\n".join(info_lines)

class LiteratureClassifier:
    """æ–‡çŒ®åˆ†ç±»å™¨"""
    
    def __init__(self, max_workers: int = None):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®è¿›ç¨‹æ•°
        if max_workers is None:
            self.max_workers = min(mp.cpu_count(), 16)  # é»˜è®¤16ä¸ªè¿›ç¨‹
        else:
            self.max_workers = max_workers
        
        print(f"ğŸ”§ å°†ä½¿ç”¨ {self.max_workers} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶å‘åˆ†ç±»")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        global OPENAI_API_KEY, OPENAI_BASE_URL
        OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
        OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        
        if not OPENAI_API_KEY:
            print("é”™è¯¯ï¼šè¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        # ç¼“å­˜æœ‰æ•ˆçš„collection keyså’Œåç§°æ˜ å°„
        self.valid_collection_keys = None
        self.name_to_key_map = None
    
    def load_valid_collection_keys(self, schema: Dict[str, Any]) -> set:
        """ä»schemaä¸­åŠ è½½æ‰€æœ‰æœ‰æ•ˆçš„collection keyså’Œåç§°æ˜ å°„"""
        if self.valid_collection_keys is not None:
            return self.valid_collection_keys
            
        valid_keys = set()
        name_to_key = {}
        
        # ä»ä¸»åˆ†ç±»ä¸­è·å–keys
        main_categories = schema.get('classification_schema', {}).get('main_categories', {})
        for main_cat_name, main_cat_info in main_categories.items():
            key = main_cat_info.get('collection_key', '')
            if key:
                valid_keys.add(key)
                name_to_key[main_cat_name] = key
            
            # ä»å­åˆ†ç±»ä¸­è·å–keys
            for sub_cat in main_cat_info.get('subcategories', []):
                sub_key = sub_cat.get('collection_key', '')
                sub_name = sub_cat.get('name', '')
                if sub_key:
                    valid_keys.add(sub_key)
                    if sub_name:
                        name_to_key[sub_name] = sub_key
        
        # ä»ç‹¬ç«‹åˆ†ç±»ä¸­è·å–keys
        independent_categories = schema.get('classification_schema', {}).get('independent_categories', {})
        for indep_cat_name, indep_cat_info in independent_categories.items():
            key = indep_cat_info.get('collection_key', '')
            if key:
                valid_keys.add(key)
                name_to_key[indep_cat_name] = key
        
        self.valid_collection_keys = valid_keys
        self.name_to_key_map = name_to_key
        
        print(f"âœ… å·²åŠ è½½ {len(valid_keys)} ä¸ªæœ‰æ•ˆåˆ†ç±»key")
        print(f"âœ… å·²å»ºç«‹ {len(name_to_key)} ä¸ªåç§°->keyæ˜ å°„")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæœ‰æ•ˆkeyæ ·æœ¬
        if valid_keys:
            sample_keys = list(valid_keys)[:5]
            print(f"   æ ·æœ¬key: {sample_keys}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªåç§°æ˜ å°„æ ·æœ¬
        if name_to_key:
            sample_mappings = list(name_to_key.items())[:3]
            print(f"   æ ·æœ¬æ˜ å°„: {sample_mappings}")
        
        return valid_keys
    
    def convert_names_to_keys(self, items: List[str]) -> List[str]:
        """å°è¯•å°†åˆ†ç±»åç§°è½¬æ¢ä¸ºcollection keys"""
        if not self.name_to_key_map:
            return items
        
        converted_keys = []
        for item in items:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æœ‰æ•ˆçš„key
            if item in self.valid_collection_keys:
                converted_keys.append(item)
            # å¦‚æœä¸æ˜¯keyï¼Œå°è¯•é€šè¿‡åç§°æ˜ å°„æŸ¥æ‰¾
            elif item in self.name_to_key_map:
                converted_key = self.name_to_key_map[item]
                converted_keys.append(converted_key)
                print(f"   ğŸ”„ è‡ªåŠ¨è½¬æ¢: '{item}' â†’ '{converted_key}'")
            else:
                # æ—¢ä¸æ˜¯æœ‰æ•ˆkeyä¹Ÿä¸æ˜¯å·²çŸ¥åç§°ï¼Œä¿æŒåŸå€¼
                converted_keys.append(item)
        
        return converted_keys
    
    def count_valid_collections(self, collection_keys_str: str) -> int:
        """è®¡ç®—æœ‰æ•ˆçš„collection keysæ•°é‡"""
        if not collection_keys_str or pd.isna(collection_keys_str):
            return 0
            
        if not self.valid_collection_keys:
            return 0
        
        # åˆ†å‰²collection keyså­—ç¬¦ä¸²ï¼ˆæ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼‰
        str_data = str(collection_keys_str)
        if ';' in str_data:
            keys = [key.strip() for key in str_data.split(';') if key.strip()]
        elif ',' in str_data:
            keys = [key.strip() for key in str_data.split(',') if key.strip()]
        else:
            keys = [str_data.strip()] if str_data.strip() else []
        
        # è®¡ç®—æœ‰æ•ˆkeysæ•°é‡
        valid_count = 0
        for key in keys:
            if key in self.valid_collection_keys:
                valid_count += 1
        
        # è°ƒè¯•ä¿¡æ¯ï¼ˆä»…åœ¨æœ‰keysæ—¶è¾“å‡ºï¼‰
        if keys and hasattr(self, '_debug_count') and self._debug_count < 3:
            print(f"   ğŸ” è°ƒè¯•æ ·æœ¬: '{collection_keys_str}' â†’ {keys} â†’ æœ‰æ•ˆæ•°é‡: {valid_count}")
            self._debug_count += 1
        
        return valid_count
    
    def load_latest_literature_info(self) -> Optional[pd.DataFrame]:
        """åŠ è½½æœ€æ–°çš„æ–‡çŒ®ä¿¡æ¯"""
        pattern = str(self.data_dir / "literature_info_*.xlsx")
        files = glob.glob(pattern)
        
        if not files:
            print("âŒ æœªæ‰¾åˆ°æ–‡çŒ®ä¿¡æ¯æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ 001_collect_literature_info.py")
            return None
        
        # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(files, key=os.path.getctime)
        print(f"ğŸ“ åŠ è½½æ–‡çŒ®ä¿¡æ¯: {latest_file}")
        
        try:
            df = pd.read_excel(latest_file, engine='openpyxl')
            print(f"âœ… å·²åŠ è½½ {len(df)} ç¯‡æ–‡çŒ®ä¿¡æ¯")
            return df
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡çŒ®ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def load_latest_classification_schema(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½æœ€æ–°çš„åˆ†ç±»æ ‡å‡†"""
        pattern = str(self.data_dir / "classification_schema_*.json")
        files = glob.glob(pattern)
        
        if not files:
            print("âŒ æœªæ‰¾åˆ°åˆ†ç±»æ ‡å‡†æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ 002_generate_classification_schema.py")
            return None
        
        # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(files, key=os.path.getctime)
        print(f"ğŸ“ åŠ è½½åˆ†ç±»æ ‡å‡†: {latest_file}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                schema = json.load(f)
            
            main_count = len(schema.get('classification_schema', {}).get('main_categories', {}))
            indep_count = len(schema.get('classification_schema', {}).get('independent_categories', {}))
            print(f"âœ… å·²åŠ è½½åˆ†ç±»æ ‡å‡†: {main_count} ä¸ªä¸»åˆ†ç±», {indep_count} ä¸ªç‹¬ç«‹åˆ†ç±»")
            
            return schema
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†ç±»æ ‡å‡†å¤±è´¥: {e}")
            return None
    
    def filter_literature_for_classification(self, df: pd.DataFrame, schema: Dict[str, Any]) -> pd.DataFrame:
        """ç­›é€‰éœ€è¦åˆ†ç±»çš„æ–‡çŒ®"""
        print("ğŸ” ç­›é€‰éœ€è¦åˆ†ç±»çš„æ–‡çŒ®...")
        
        # å…ˆåŠ è½½æœ‰æ•ˆçš„collection keys
        self.load_valid_collection_keys(schema)
        self._debug_count = 0  # é‡ç½®è°ƒè¯•è®¡æ•°å™¨
        
        # æ£€æŸ¥DataFrameä¸­çš„å­—æ®µå
        print("ğŸ” æ£€æŸ¥æ•°æ®å­—æ®µ...")
        print(f"   DataFrameå­—æ®µ: {list(df.columns)}")
        
        # æŸ¥æ‰¾æ­£ç¡®çš„collection keyså­—æ®µå
        possible_fields = ['collections_keys', 'collection_keys', 'collections', 'collection_names']
        collection_field = None
        for field in possible_fields:
            if field in df.columns:
                collection_field = field
                break
        
        if not collection_field:
            print("âš ï¸ æœªæ‰¾åˆ°collectionç›¸å…³å­—æ®µï¼Œå°†æ‰€æœ‰æ–‡çŒ®è§†ä¸ºéœ€è¦åˆ†ç±»")
            valid_counts = pd.Series([0] * len(df))
        else:
            print(f"   ä½¿ç”¨å­—æ®µ: {collection_field}")
            
            # åˆ†æå‡ ä¸ªæ ·æœ¬æ•°æ®
            sample_data = df[collection_field].head(3).tolist()
            print(f"   æ ·æœ¬æ•°æ®: {sample_data}")
            
            # åˆ†æç°æœ‰åˆ†ç±»æƒ…å†µ
            print("ğŸ“Š åˆ†æç°æœ‰åˆ†ç±»æƒ…å†µ...")
            valid_counts = df.apply(lambda row: self.count_valid_collections(row.get(collection_field, '')), axis=1)
        
        no_valid_classifications = (valid_counts == 0).sum()
        one_valid_classification = (valid_counts == 1).sum()  
        two_or_more_valid_classifications = (valid_counts >= 2).sum()
        
        print(f"   æ— æœ‰æ•ˆåˆ†ç±»: {no_valid_classifications} ç¯‡")
        print(f"   1ä¸ªæœ‰æ•ˆåˆ†ç±»: {one_valid_classification} ç¯‡") 
        print(f"   2ä¸ªæˆ–ä»¥ä¸Šæœ‰æ•ˆåˆ†ç±»: {two_or_more_valid_classifications} ç¯‡")
        
        # è¿‡æ»¤æ¡ä»¶
        if collection_field:
            filtered_df = df[
                # æ’é™¤æ²¡æœ‰æ ‡é¢˜çš„
                (df['title'].notna() & (df['title'] != '') & (df['title'] != 'æ— æ ‡é¢˜')) &
                # åªå¤„ç†å®Œå…¨æ²¡æœ‰æœ‰æ•ˆåˆ†ç±»çš„æ–‡çŒ®
                (valid_counts == 0)
            ].copy()
        else:
            # å¦‚æœæ²¡æœ‰collectionå­—æ®µï¼ŒåªæŒ‰æ ‡é¢˜ç­›é€‰
            filtered_df = df[
                (df['title'].notna() & (df['title'] != '') & (df['title'] != 'æ— æ ‡é¢˜'))
            ].copy()
        
        print(f"\nğŸ“Š ç­›é€‰ç»“æœ:")
        print(f"   æ€»æ–‡çŒ®æ•°: {len(df)}")
        print(f"   å¾…åˆ†ç±»æ–‡çŒ®æ•°: {len(filtered_df)}")
        
        if collection_field:
            skipped_count = one_valid_classification + two_or_more_valid_classifications
            print(f"   å·²è·³è¿‡ï¼ˆæœ‰æ•ˆåˆ†ç±»>=1ï¼‰: {skipped_count} ç¯‡")
        else:
            print(f"   æ— collectionå­—æ®µï¼Œä»…æŒ‰æ ‡é¢˜ç­›é€‰")
        
        if len(filtered_df) > 0:
            print(f"\nğŸ“š å¾…åˆ†ç±»æ–‡çŒ®ç±»å‹åˆ†å¸ƒ:")
            type_counts = filtered_df['item_type'].value_counts().head(5)
            for item_type, count in type_counts.items():
                print(f"     - {item_type}: {count} ç¯‡")
        
        return filtered_df
    
    def classify_literature_batch(self, literature_df: pd.DataFrame, schema: Dict[str, Any], 
                                 limit: Optional[int] = None, start: int = 0) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ç±»æ–‡çŒ®"""
        
        # ç¡®å®šå¤„ç†èŒƒå›´
        total_count = len(literature_df)
        if limit is None:
            limit = total_count
        
        end_index = min(start + limit, total_count)
        selected_df = literature_df.iloc[start:end_index]
        
        print(f"ğŸš€ å¼€å§‹åˆ†ç±»æ–‡çŒ®:")
        print(f"   å¤„ç†èŒƒå›´: ç¬¬ {start+1} åˆ°ç¬¬ {end_index} ç¯‡")
        print(f"   æ€»æ•°: {len(selected_df)} ç¯‡")
        print(f"   å¹¶å‘è¿›ç¨‹: {self.max_workers} ä¸ª")
        
        # å‡†å¤‡ä»»åŠ¡æ•°æ®
        tasks = []
        for idx, row in selected_df.iterrows():
            literature_info = row.to_dict()
            tasks.append((literature_info, schema['classification_schema'], idx % self.max_workers, self))
        
        # å¤šè¿›ç¨‹æ‰§è¡Œ
        results = []
        
        with ProcessPoolExecutor(
            max_workers=self.max_workers,
            initializer=init_worker
        ) as executor:
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(classify_single_literature, task): i 
                for i, task in enumerate(tasks)
            }
            
            # æ”¶é›†ç»“æœ
            with tqdm(total=len(tasks), desc="ä¸¤é˜¶æ®µåˆ†ç±»è¿›åº¦", unit="ç¯‡") as pbar:
                for future in as_completed(future_to_task):
                    try:
                        result = future.result()
                        results.append(result)
                        pbar.update(1)
                        
                        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                        if result['success']:
                            recommended = result.get('recommended_collections', [])
                            if recommended:
                                pbar.set_postfix_str(f"æœ€æ–°: {result['title'][:15]}... â†’ {len(recommended)}ä¸ªåˆ†ç±»")
                        else:
                            pbar.set_postfix_str(f"å¤±è´¥: {result.get('error', 'Unknown')}")
                        
                    except Exception as e:
                        task_idx = future_to_task[future]
                        task_info = tasks[task_idx]
                        results.append({
                            'success': False,
                            'item_key': task_info[0].get('item_key', ''),
                            'title': task_info[0].get('title', ''),
                            'worker_id': task_info[2],
                            'error': f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}',
                            'main_response': '',
                            'sub_responses': []
                        })
                        pbar.update(1)
        
        return results
    
    def save_classification_results(self, results: List[Dict[str, Any]], 
                                  literature_df: pd.DataFrame) -> str:
        """ä¿å­˜åˆ†ç±»ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"classification_results_{timestamp}.xlsx"
        filepath = self.data_dir / filename
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ†ç±»ç»“æœåˆ° {filepath}...")
        
        # æ„å»ºç»“æœDataFrame
        result_data = []
        
        for result in results:
            item_key = result['item_key']
            
            # ä»åŸå§‹æ–‡çŒ®ä¿¡æ¯ä¸­è·å–å®Œæ•´ä¿¡æ¯
            literature_row = literature_df[literature_df['item_key'] == item_key]
            if not literature_row.empty:
                lit_info = literature_row.iloc[0].to_dict()
            else:
                lit_info = {}
            
            # æ„å»ºç»“æœè¡Œ
            result_row = {
                'item_key': item_key,
                'title': result['title'],
                'item_type': lit_info.get('item_type', ''),
                'authors': lit_info.get('authors', ''),
                'publication_title': lit_info.get('publication_title', ''),
                'date': lit_info.get('date', ''),
                'doi': lit_info.get('doi', ''),
                'abstract': lit_info.get('abstract', ''),
                'current_collections_count': lit_info.get('collections_count', 0),
                'classification_success': result['success'],
                'recommended_collection_keys': '; '.join(result.get('recommended_collection_keys', [])) if result['success'] else '',
                'recommended_collections': '; '.join(result.get('recommended_collections', [])) if result['success'] else '',
                'recommended_count': len(result.get('recommended_collections', [])) if result['success'] and result.get('recommended_collections') else 0,
                'main_analysis': result.get('main_analysis', ''),
                'error_message': result.get('error', ''),
                'worker_id': result.get('worker_id', ''),
                'main_response': result.get('main_response', ''),
                'sub_responses_count': len(result.get('sub_responses', [])) if result['success'] and result.get('sub_responses') else 0
            }
            
            result_data.append(result_row)
        
        # ä¿å­˜åˆ°Excel
        result_df = pd.DataFrame(result_data)
        result_df.to_excel(filepath, index=False, engine='openpyxl')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        successful_results = result_df[result_df['classification_success'] == True]
        failed_results = result_df[result_df['classification_success'] == False]
        
        print(f"\nğŸ“Š ä¸¤é˜¶æ®µåˆ†ç±»ç»“æœç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸåˆ†ç±»: {len(successful_results)} ç¯‡")
        print(f"   âŒ åˆ†ç±»å¤±è´¥: {len(failed_results)} ç¯‡")
        
        if len(successful_results) > 0:
            print(f"   ğŸ“‚ å¹³å‡æ¨èåˆ†ç±»æ•°: {successful_results['recommended_count'].mean():.1f}")
            print(f"   ğŸ”„ å¹³å‡å­åˆ†ç±»å“åº”æ•°: {successful_results['sub_responses_count'].mean():.1f}")
            
            # ç»Ÿè®¡æ¨èåˆ†ç±»
            all_recommended_keys = []
            all_recommended_names = []
            for keys, names in zip(successful_results['recommended_collection_keys'], 
                                 successful_results['recommended_collections']):
                if keys:
                    all_recommended_keys.extend([key.strip() for key in keys.split(';')])
                if names:
                    all_recommended_names.extend([name.strip() for name in names.split(';')])
            
            if all_recommended_names:
                from collections import Counter
                category_counts = Counter(all_recommended_names)
                print(f"\nğŸ“‚ çƒ­é—¨æ¨èåˆ†ç±»:")
                for category, count in category_counts.most_common(10):
                    print(f"     - {category}: {count} ç¯‡")
            
            print(f"\nğŸ”‘ ä½¿ç”¨çš„åˆ†ç±»keyæ€»æ•°: {len(set(all_recommended_keys))}")
        
        if len(failed_results) > 0:
            print(f"\nâŒ å¤±è´¥åŸå› ç»Ÿè®¡:")
            error_counts = failed_results['error_message'].value_counts()
            for error, count in error_counts.head(5).items():
                print(f"     - {error}: {count} ç¯‡")
        
        print(f"\nâœ… åˆ†ç±»ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return str(filepath)
    
    def classify_and_save(self, limit: Optional[int] = None, start: int = 0) -> str:
        """æ‰§è¡Œåˆ†ç±»å¹¶ä¿å­˜ç»“æœ"""
        print("ğŸš€ å¼€å§‹æ–‡çŒ®åˆ†ç±»ä»»åŠ¡...")
        
        # åŠ è½½æ•°æ®
        literature_df = self.load_latest_literature_info()
        if literature_df is None:
            return ""
        
        schema = self.load_latest_classification_schema()
        if schema is None:
            return ""
        
        # ç­›é€‰éœ€è¦åˆ†ç±»çš„æ–‡çŒ®
        filtered_df = self.filter_literature_for_classification(literature_df, schema)
        if len(filtered_df) == 0:
            print("âœ… æ²¡æœ‰éœ€è¦åˆ†ç±»çš„æ–‡çŒ®")
            return ""
        
        # æ‰§è¡Œåˆ†ç±»
        results = self.classify_literature_batch(filtered_df, schema, limit, start)
        
        # ä¿å­˜ç»“æœ
        result_file = self.save_classification_results(results, literature_df)
        
        return result_file


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– Zoteroæ–‡çŒ®æ™ºèƒ½åˆ†ç±»å·¥å…· - 003")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ['OPENAI_API_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"\nâŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("\nè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š")
        print("export OPENAI_API_KEY='ä½ çš„OpenAI APIå¯†é’¥'")
        print("export OPENAI_BASE_URL='ä½ çš„OpenAI Base URL' (å¯é€‰)")
        return 1
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    limit = None
    start = 0
    max_workers = None
    
    if len(sys.argv) > 1:
        try:
            limit = int(sys.argv[1])
        except ValueError:
            print("âŒ æ— æ•ˆçš„é™åˆ¶æ•°é‡å‚æ•°")
            return 1
    
    if len(sys.argv) > 2:
        try:
            start = int(sys.argv[2])
        except ValueError:
            print("âŒ æ— æ•ˆçš„èµ·å§‹ä½ç½®å‚æ•°")
            return 1
    
    if len(sys.argv) > 3:
        try:
            max_workers = int(sys.argv[3])
        except ValueError:
            print("âŒ æ— æ•ˆçš„è¿›ç¨‹æ•°å‚æ•°")
            return 1
    
    try:
        # åˆ›å»ºåˆ†ç±»å™¨
        classifier = LiteratureClassifier(max_workers=max_workers)
        
        # æ‰§è¡Œåˆ†ç±»
        if limit is not None:
            print(f"ğŸ“‹ å°†åˆ†ç±» {limit} ç¯‡æ–‡çŒ®ï¼Œä»ç¬¬ {start+1} ç¯‡å¼€å§‹")
        else:
            print(f"ğŸ“‹ å°†åˆ†ç±»æ‰€æœ‰æ–‡çŒ®ï¼Œä»ç¬¬ {start+1} ç¯‡å¼€å§‹")
        
        result_file = classifier.classify_and_save(limit=limit, start=start)
        
        if result_file:
            print(f"\nğŸ‰ åˆ†ç±»å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
            print(f"   1. æ£€æŸ¥åˆ†ç±»ç»“æœæ–‡ä»¶")
            print(f"   2. è¿è¡Œ: python 004_apply_classification.py")
        else:
            print("âŒ åˆ†ç±»å¤±è´¥")
            return 1
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main()) 