import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any
from openai import OpenAI
from anthropic import Anthropic
from tenacity import retry, stop_after_attempt, wait_exponential
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯æ¥å£ï¼Œå¸¦ç¼“å­˜æœºåˆ¶"""
    
    def __init__(self, model_name: str, api_key: str, base_url: str):
        self.model_name = model_name
        self.api_key = os.environ.get("OPENAI_API_KEY")
        self.base_url = os.environ.get("OPENAI_BASE_URL")
        
        # åˆå§‹åŒ–ç¼“å­˜ç›®å½•
        self.cache_dir = Path("./.cache/llm_responses")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        if "claude" in model_name.lower():
            self.client_type = "anthropic"
            # Claudeéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨OpenAIå…¼å®¹çš„æ¥å£
            self.client = OpenAI(api_key=api_key, base_url=base_url)
        else:
            self.client_type = "openai"
            self.client = OpenAI(api_key=api_key, base_url=base_url)
    
    def _generate_cache_key(self, prompt: str, system_prompt: Optional[str] = None, 
                           max_tokens: int = 4096, temperature: float = 0.7, 
                           tools: Optional[List[Dict]] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®å€¼"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—å…¸
        cache_data = {
            "model": self.model_name,
            "prompt": prompt,
            "system_prompt": system_prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "tools": tools
        }
        
        # ä½¿ç”¨JSONåºåˆ—åŒ–å¹¶è®¡ç®—MD5å“ˆå¸Œ
        cache_string = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        cache_hash = hashlib.md5(cache_string.encode('utf-8')).hexdigest()
        return cache_hash
    
    def _get_cached_response(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–å›å¤"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    logger.warning(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜å›å¤ (model: {self.model_name}, cache: {cache_key[:8]}...)")
                    return cache_data.get("response")
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {str(e)}")
        return None
    
    def _save_cached_response(self, cache_key: str, prompt: str, system_prompt: Optional[str], 
                             response: Dict[str, Any], max_tokens: int, temperature: float,
                             tools: Optional[List[Dict]] = None) -> None:
        """ä¿å­˜å›å¤åˆ°ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            cache_data = {
                "model": self.model_name,
                "prompt": prompt,
                "system_prompt": system_prompt,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "response": response,
                "timestamp": datetime.now().isoformat()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ (model: {self.model_name}, cache: {cache_key[:8]}...)")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def generate(self, prompt: str, system_prompt: Optional[str] = None, 
                 max_tokens: int = 4096, temperature: float = 0.7,
                 tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå›å¤ï¼Œå¸¦ç¼“å­˜æœºåˆ¶"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(prompt, system_prompt, max_tokens, temperature, tools)
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_response = self._get_cached_response(cache_key)
        if cached_response is not None:
            return cached_response
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨API
        try:
            # ç»Ÿä¸€ä½¿ç”¨OpenAIæ¥å£æ ¼å¼ï¼ˆClaudeä¹Ÿä½¿ç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # æ„å»ºAPIè°ƒç”¨å‚æ•°
            api_params = {
                "model": self.model_name,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            # å¦‚æœæä¾›äº†å·¥å…·ï¼Œæ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ
            if tools:
                api_params["tools"] = tools
                api_params["tool_choice"] = "auto"
            
            logger.warning(f"ğŸŒ è°ƒç”¨API (model: {self.model_name}, cache: {cache_key[:8]}...)")
            response = self.client.chat.completions.create(**api_params)
            
            # è§£æå“åº”
            message = response.choices[0].message
            result = {
                "content": message.content,
                "tool_calls": []
            }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if hasattr(message, 'tool_calls') and message.tool_calls:
                for tool_call in message.tool_calls:
                    result["tool_calls"].append({
                        "id": tool_call.id,
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    })
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_cached_response(cache_key, prompt, system_prompt, result, max_tokens, temperature, tools)
            
            return result
                
        except Exception as e:
            logger.error(f"Error generating response with {self.model_name}: {str(e)}")
            raise
    
    def generate_text(self, prompt: str, system_prompt: Optional[str] = None,
                     max_tokens: int = 4096, temperature: float = 0.7) -> str:
        """å‘åå…¼å®¹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•"""  
        result = self.generate(prompt, system_prompt, max_tokens, temperature)
        return result.get("content", "")