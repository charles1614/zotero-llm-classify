#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Client - ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯
æ”¯æŒå¤šç§LLM APIï¼ŒåŒ…æ‹¬OpenAIå…¼å®¹æ¥å£å’ŒGeminiç›´æ¥æ¥å£
"""

import os
import time
import json
import hashlib
import httpx
from typing import Dict, Any, Optional, List
import logging
from datetime import datetime
from pathlib import Path
from collections import deque
from openai import OpenAI
try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None
from tenacity import retry, stop_after_attempt, wait_exponential
try:
    from google import genai
except ImportError:
    genai = None

# å¯¼å…¥é…ç½®
from config import get_llm_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RateLimiter:
    """ç®€å•çš„é€Ÿç‡é™åˆ¶å™¨ï¼ŒåŸºäºæ»‘åŠ¨çª—å£"""
    
    def __init__(self, max_requests: int, window_seconds: int = 60):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests = deque()
    
    def can_proceed(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­è¯·æ±‚"""
        now = time.time()
        
        # ç§»é™¤çª—å£å¤–çš„è¯·æ±‚
        while self.requests and now - self.requests[0] > self.window_seconds:
            self.requests.popleft()
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
        if len(self.requests) >= self.max_requests:
            return False
        
        return True
    
    def record_request(self):
        """è®°å½•ä¸€ä¸ªè¯·æ±‚"""
        now = time.time()
        self.requests.append(now)
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥ç»§ç»­è¯·æ±‚"""
        while not self.can_proceed():
            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            if self.requests:
                oldest_request = self.requests[0]
                wait_time = self.window_seconds - (time.time() - oldest_request)
                if wait_time > 0:
                    logger.info(f"â³ é€Ÿç‡é™åˆ¶: ç­‰å¾… {wait_time:.1f} ç§’...")
                    time.sleep(wait_time)
            else:
                break
        
        self.record_request()

class LLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯æ¥å£ï¼Œå¸¦ç¼“å­˜æœºåˆ¶"""
    
    def __init__(self, model_name: str = None, api_key: str = None, base_url: str = None):
        # è·å–é…ç½®
        config = get_llm_config()
        
        self.model_name = model_name or config.model
        self.api_key = api_key or config.api_key
        
        # æ ¹æ®APIç±»å‹é€‰æ‹©æ­£ç¡®çš„base_url
        if config.api_type == 'gemini-direct':
            self.base_url = base_url or config.gemini_base_url
        else:
            self.base_url = base_url or config.base_url
        
        # åˆå§‹åŒ–ç¼“å­˜ç›®å½•
        self.cache_dir = Path("./.cache/llm_responses")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®è¶…æ—¶å‚æ•°
        timeout_config = httpx.Timeout(config.timeout, connect=config.connect_timeout)
        
        # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        self.rate_limiter = None
        
        # è®¾ç½®é€Ÿç‡é™åˆ¶
        if config.rpm_limit > 0:
            self.rate_limiter = RateLimiter(max_requests=config.rpm_limit, window_seconds=60)
            logger.info(f"ğŸ”’ ä¸º {self.model_name} å¯ç”¨é€Ÿç‡é™åˆ¶: {config.rpm_limit} RPM")
        
        if "claude" in self.model_name.lower():
            self.client_type = "anthropic"
            # Claudeéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨OpenAIå…¼å®¹çš„æ¥å£
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url, timeout=timeout_config)
        elif "gemini" in self.model_name.lower():
            self.client_type = "gemini"
            # æ£€æŸ¥æ˜¯å¦æ˜¯å®˜æ–¹Gemini API
            if "generativelanguage.googleapis.com" in self.base_url:
                # å®˜æ–¹Gemini API - ä½¿ç”¨google.generativeaiåº“
                self.client = genai.Client(api_key=config.gemini_api_key)
                self.gemini_base_url = self.base_url
                self.gemini_api_key = config.gemini_api_key
            else:
                # ä»£ç†API - ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url, timeout=timeout_config)
                self.gemini_base_url = self.base_url
                self.gemini_api_key = config.gemini_api_key
        else:
            self.client_type = "openai"
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url, timeout=timeout_config)
    
    def _generate_cache_key(self, prompt: str, system_prompt: Optional[str] = None, 
                           max_tokens: int = 4096, temperature: float = 0.7, 
                           tools: Optional[List[Dict]] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®å€¼"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—å…¸
        cache_data = {
            "model": self.model_name,
            "prompt": prompt,
            "system_prompt": system_prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "tools": tools
        }
        
        # ä½¿ç”¨JSONåºåˆ—åŒ–å¹¶è®¡ç®—MD5å“ˆå¸Œ
        cache_string = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        cache_hash = hashlib.md5(cache_string.encode('utf-8')).hexdigest()
        return cache_hash
    
    def _get_cached_response(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–å›å¤"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    logger.warning(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜å›å¤ (model: {self.model_name}, cache: {cache_key[:8]}...)")
                    return cache_data.get("response")
            except Exception as e:
                logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {str(e)}")
        return None
    
    def _save_cached_response(self, cache_key: str, prompt: str, system_prompt: Optional[str], 
                             response: Dict[str, Any], max_tokens: int, temperature: float,
                             tools: Optional[List[Dict]] = None) -> None:
        """ä¿å­˜å›å¤åˆ°ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            cache_data = {
                "model": self.model_name,
                "prompt": prompt,
                "system_prompt": system_prompt,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "response": response,
                "timestamp": datetime.now().isoformat()
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            logger.debug(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ (model: {self.model_name}, cache: {cache_key[:8]}...)")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")

    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))
    def generate(self, prompt: str, system_prompt: Optional[str] = None, 
                 max_tokens: int = 4096, temperature: float = 0.7,
                 tools: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå›å¤ï¼Œå¸¦ç¼“å­˜æœºåˆ¶"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(prompt, system_prompt, max_tokens, temperature, tools)
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_response = self._get_cached_response(cache_key)
        if cached_response is not None:
            return cached_response
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨API
        try:
            # åº”ç”¨é€Ÿç‡é™åˆ¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if self.rate_limiter:
                self.rate_limiter.wait_if_needed()
            
            logger.warning(f"ğŸŒ è°ƒç”¨API (model: {self.model_name}, cache: {cache_key[:8]}...)")
            
            if self.client_type == "gemini":
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®˜æ–¹Gemini API
                if "generativelanguage.googleapis.com" in self.gemini_base_url:
                    # å®˜æ–¹Gemini API
                    response = self._call_official_gemini_api(prompt, system_prompt, max_tokens, temperature)
                    result = {
                        "content": response.get("content", ""),
                        "tool_calls": []
                    }
                else:
                    # ä»£ç†API - OpenAIå…¼å®¹æ¥å£
                    messages = []
                    if system_prompt:
                        messages.append({"role": "system", "content": system_prompt})
                    messages.append({"role": "user", "content": prompt})
                    
                    # æ„å»ºAPIè°ƒç”¨å‚æ•°
                    api_params = {
                        "model": self.model_name,
                        "messages": messages,
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                    
                    # å¦‚æœæä¾›äº†å·¥å…·ï¼Œæ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ
                    if tools:
                        api_params["tools"] = tools
                        api_params["tool_choice"] = "auto"
                    
                    response = self.client.chat.completions.create(**api_params)
                    
                    # OpenAIå…¼å®¹æ¥å£å“åº”æ ¼å¼
                    message = response.choices[0].message
                    result = {
                        "content": message.content,
                        "tool_calls": []
                    }
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if hasattr(message, 'tool_calls') and message.tool_calls:
                        for tool_call in message.tool_calls:
                            result["tool_calls"].append({
                                "id": tool_call.id,
                                "function": {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments
                                }
                            })
            else:
                # OpenAIå…¼å®¹æ¥å£
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                # æ„å»ºAPIè°ƒç”¨å‚æ•°
                api_params = {
                    "model": self.model_name,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
                
                # å¦‚æœæä¾›äº†å·¥å…·ï¼Œæ·»åŠ å·¥å…·è°ƒç”¨æ”¯æŒ
                if tools:
                    api_params["tools"] = tools
                    api_params["tool_choice"] = "auto"
                
                response = self.client.chat.completions.create(**api_params)
                
                # OpenAIå…¼å®¹æ¥å£å“åº”æ ¼å¼
                message = response.choices[0].message
                result = {
                    "content": message.content,
                    "tool_calls": []
                }
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    for tool_call in message.tool_calls:
                        result["tool_calls"].append({
                            "id": tool_call.id,
                            "function": {
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments
                            }
                        })
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_cached_response(cache_key, prompt, system_prompt, result, max_tokens, temperature, tools)
            
            return result
                
        except Exception as e:
            # æ ¹æ®å®¢æˆ·ç«¯ç±»å‹æä¾›æ›´å‡†ç¡®çš„é”™è¯¯ä¿¡æ¯
            if self.client_type == "gemini":
                if self.client is None:
                    logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥ (model: {self.model_name}): {str(e)}")
                else:
                    logger.error(f"Geminiå…¼å®¹æ¥å£è°ƒç”¨å¤±è´¥ (model: {self.model_name}): {str(e)}")
            elif self.client_type == "anthropic":
                logger.error(f"Claude APIè°ƒç”¨å¤±è´¥ (model: {self.model_name}): {str(e)}")
            else:
                logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥ (model: {self.model_name}): {str(e)}")
            raise
    
    def _call_official_gemini_api(self, prompt: str, system_prompt: Optional[str] = None, 
                                 max_tokens: int = 4096, temperature: float = 0.7) -> Dict[str, Any]:
        """è°ƒç”¨å®˜æ–¹Gemini APIä½¿ç”¨google.generativeaiåº“"""
        try:
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"System: {system_prompt}\n\n{prompt}"
            
            # ä½¿ç”¨genai.Client()è°ƒç”¨API
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt
            )
            
            # æå–æ–‡æœ¬å†…å®¹
            if response.text:
                return {
                    "content": response.text
                }
            else:
                return {
                    "content": ""
                }
                
        except Exception as e:
            logger.error(f"å®˜æ–¹Gemini APIè°ƒç”¨å¤±è´¥ (model: {self.model_name}): {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    def generate_text(self, prompt: str, system_prompt: Optional[str] = None,
                     max_tokens: int = 4096, temperature: float = 0.7) -> str:
        """å‘åå…¼å®¹çš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•"""  
        result = self.generate(prompt, system_prompt, max_tokens, temperature)
        return result.get("content", "")