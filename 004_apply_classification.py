#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
004 - Apply Classification
æ‰¹é‡åº”ç”¨åˆ†ç±»ç»“æœåˆ°Zoteroï¼Œæ”¯æŒå¤šè¿›ç¨‹å¹¶å‘
"""

import os
import sys
import json
import pandas as pd
import multiprocessing as mp
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import glob
import time
import threading
import queue

# å¯¼å…¥å·²æœ‰çš„æ¨¡å—
from main import ZoteroManager

# å…¨å±€Zoteroé…ç½®ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
ZOTERO_USER_ID = None
ZOTERO_API_KEY = None

# å…¨å±€é”ç”¨äºé˜²æ­¢APIé™åˆ¶
api_lock = threading.Lock()

def init_worker():
    """å·¥ä½œè¿›ç¨‹åˆå§‹åŒ–å‡½æ•°"""
    global ZOTERO_USER_ID, ZOTERO_API_KEY
    ZOTERO_USER_ID = os.getenv('ZOTERO_USER_ID')
    ZOTERO_API_KEY = os.getenv('ZOTERO_API_KEY')

def apply_single_classification(args):
    """åº”ç”¨å•ç¯‡æ–‡çŒ®åˆ†ç±»çš„å·¥ä½œå‡½æ•°"""
    classification_row, worker_id, rate_limit_delay = args
    
    try:
        # åœ¨å·¥ä½œè¿›ç¨‹ä¸­åˆ›å»ºZoteroå®¢æˆ·ç«¯
        zotero = ZoteroManager(ZOTERO_USER_ID, ZOTERO_API_KEY)
        
        # è§£ææ¨èåˆ†ç±»keys
        item_key = classification_row['item_key']
        recommended_collection_keys_str = classification_row.get('recommended_collection_keys', '')
        
        if not recommended_collection_keys_str or pd.isna(recommended_collection_keys_str):
            return {
                'success': False,
                'item_key': item_key,
                'title': classification_row.get('title', ''),
                'worker_id': worker_id,
                'error': 'æ²¡æœ‰æ¨èåˆ†ç±»keys',
                'applied_collections': []
            }
        
        # åˆ†å‰²æ¨èåˆ†ç±»keys
        recommended_collection_keys = [key.strip() for key in recommended_collection_keys_str.split(';') if key.strip()]
        
        if not recommended_collection_keys:
            return {
                'success': False,
                'item_key': item_key,
                'title': classification_row.get('title', ''),
                'worker_id': worker_id,
                'error': 'æ¨èåˆ†ç±»keysä¸ºç©º',
                'applied_collections': []
            }
        
        # åº”ç”¨åˆ†ç±»
        applied_collections = []
        failed_collections = []
        
        for collection_key in recommended_collection_keys:
            
            # æ·»åŠ é€Ÿç‡é™åˆ¶å»¶è¿Ÿ
            if rate_limit_delay > 0:
                time.sleep(rate_limit_delay)
            
            # å°è¯•æ·»åŠ åˆ°åˆ†ç±»
            try:
                success = zotero.add_item_to_collection(item_key, collection_key)
                if success:
                    applied_collections.append(collection_key)
                else:
                    failed_collections.append(f"{collection_key}(APIå¤±è´¥)")
                    
            except Exception as e:
                failed_collections.append(f"{collection_key}(å¼‚å¸¸: {str(e)})")
        
        return {
            'success': len(applied_collections) > 0,
            'item_key': item_key,
            'title': classification_row.get('title', ''),
            'worker_id': worker_id,
            'applied_collections': applied_collections,
            'failed_collections': failed_collections,
            'error': '; '.join(failed_collections) if failed_collections else ''
        }
        
    except Exception as e:
        return {
            'success': False,
            'item_key': classification_row.get('item_key', ''),
            'title': classification_row.get('title', ''),
            'worker_id': worker_id,
            'error': f'å¤„ç†å¼‚å¸¸: {str(e)}',
            'applied_collections': [],
            'failed_collections': []
        }

class ClassificationApplier:
    """åˆ†ç±»åº”ç”¨å™¨"""
    
    def __init__(self, max_workers: int = None, rate_limit_delay: float = 0.1):
        """åˆå§‹åŒ–åº”ç”¨å™¨"""
        self.data_dir = Path("data")
        self.data_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®è¿›ç¨‹æ•°
        if max_workers is None:
            # APIè°ƒç”¨æ¯”CPUå¯†é›†ä»»åŠ¡éœ€è¦æ›´å°‘çš„è¿›ç¨‹
            self.max_workers = min(mp.cpu_count() // 2, 4)  # æœ€å¤š4ä¸ªè¿›ç¨‹
        else:
            self.max_workers = max_workers
        
        # APIé€Ÿç‡é™åˆ¶å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.rate_limit_delay = rate_limit_delay
        
        print(f"ğŸ”§ å°†ä½¿ç”¨ {self.max_workers} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶å‘åº”ç”¨")
        print(f"â±ï¸ APIé€Ÿç‡é™åˆ¶å»¶è¿Ÿ: {self.rate_limit_delay}ç§’")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        global ZOTERO_USER_ID, ZOTERO_API_KEY
        ZOTERO_USER_ID = os.getenv('ZOTERO_USER_ID')
        ZOTERO_API_KEY = os.getenv('ZOTERO_API_KEY')
        
        if not ZOTERO_USER_ID or not ZOTERO_API_KEY:
            print("é”™è¯¯ï¼šè¯·è®¾ç½®ZOTERO_USER_IDå’ŒZOTERO_API_KEYç¯å¢ƒå˜é‡")
            sys.exit(1)
        
        # åˆ›å»ºZoteroå®¢æˆ·ç«¯
        self.zotero = ZoteroManager(ZOTERO_USER_ID, ZOTERO_API_KEY)
    

    def load_latest_classification_results(self) -> Optional[pd.DataFrame]:
        """åŠ è½½æœ€æ–°çš„åˆ†ç±»ç»“æœ"""
        pattern = str(self.data_dir / "classification_results_*.xlsx")
        files = glob.glob(pattern)
        
        if not files:
            print("âŒ æœªæ‰¾åˆ°åˆ†ç±»ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ 003_classify_literature.py")
            return None
        
        # é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(files, key=os.path.getctime)
        print(f"ğŸ“ åŠ è½½åˆ†ç±»ç»“æœ: {latest_file}")
        
        try:
            df = pd.read_excel(latest_file, engine='openpyxl')
            print(f"âœ… å·²åŠ è½½ {len(df)} æ¡åˆ†ç±»ç»“æœ")
            return df
        except Exception as e:
            print(f"âŒ åŠ è½½åˆ†ç±»ç»“æœå¤±è´¥: {e}")
            return None
    
    def filter_results_for_application(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç­›é€‰éœ€è¦åº”ç”¨çš„åˆ†ç±»ç»“æœ"""
        print("ğŸ” ç­›é€‰éœ€è¦åº”ç”¨çš„åˆ†ç±»ç»“æœ...")
        
        # ç­›é€‰æˆåŠŸåˆ†ç±»ä¸”æœ‰æ¨èåˆ†ç±»keysçš„ç»“æœ
        filtered_df = df[
            (df['classification_success'] == True) &
            (df['recommended_collection_keys'].notna()) &
            (df['recommended_collection_keys'] != '') &
            (df['recommended_count'] > 0)
        ].copy()
        
        print(f"ğŸ“Š ç­›é€‰ç»“æœ:")
        print(f"   æ€»ç»“æœæ•°: {len(df)}")
        print(f"   æˆåŠŸåˆ†ç±»æ•°: {len(df[df['classification_success'] == True])}")
        print(f"   æœ‰æ¨èåˆ†ç±»keysæ•°: {len(filtered_df)}")
        
        if len(filtered_df) > 0:
            print(f"   å¹³å‡æ¨èåˆ†ç±»æ•°: {filtered_df['recommended_count'].mean():.1f}")
            
            # ç»Ÿè®¡æ¨èåˆ†ç±»
            all_recommended = []
            for recommendations in filtered_df['recommended_collections']:
                if recommendations:
                    all_recommended.extend([cat.strip() for cat in recommendations.split(';')])
            
            if all_recommended:
                from collections import Counter
                category_counts = Counter(all_recommended)
                print(f"\nğŸ“‚ å³å°†åº”ç”¨çš„åˆ†ç±»ç»Ÿè®¡:")
                for category, count in category_counts.most_common(10):
                    print(f"     - {category}: {count} ç¯‡")
        
        return filtered_df
    
    def apply_classifications_batch(self, results_df: pd.DataFrame, 
                                  limit: Optional[int] = None, start: int = 0) -> List[Dict[str, Any]]:
        """æ‰¹é‡åº”ç”¨åˆ†ç±»"""
        
        # ç¡®å®šå¤„ç†èŒƒå›´
        total_count = len(results_df)
        if limit is None:
            limit = total_count
        
        end_index = min(start + limit, total_count)
        selected_df = results_df.iloc[start:end_index]
        
        print(f"ğŸš€ å¼€å§‹åº”ç”¨åˆ†ç±»:")
        print(f"   å¤„ç†èŒƒå›´: ç¬¬ {start+1} åˆ°ç¬¬ {end_index} æ¡ç»“æœ")
        print(f"   æ€»æ•°: {len(selected_df)} æ¡")
        print(f"   å¹¶å‘è¿›ç¨‹: {self.max_workers} ä¸ª")
        
        # å‡†å¤‡ä»»åŠ¡æ•°æ®
        tasks = []
        for idx, row in selected_df.iterrows():
            tasks.append((row, idx % self.max_workers, self.rate_limit_delay))
        
        # å¤šè¿›ç¨‹æ‰§è¡Œ
        results = []
        
        with ProcessPoolExecutor(
            max_workers=self.max_workers,
            initializer=init_worker
        ) as executor:
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(apply_single_classification, task): i 
                for i, task in enumerate(tasks)
            }
            
            # æ”¶é›†ç»“æœ
            with tqdm(total=len(tasks), desc="åº”ç”¨è¿›åº¦", unit="æ¡") as pbar:
                for future in as_completed(future_to_task):
                    try:
                        result = future.result()
                        results.append(result)
                        pbar.update(1)
                        
                        # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                        if result['success']:
                            applied_count = len(result.get('applied_collections', []))
                            if applied_count > 0:
                                pbar.set_postfix_str(f"æœ€æ–°: {result['title'][:20]}... â†’ {applied_count}ä¸ªåˆ†ç±»")
                        
                    except Exception as e:
                        task_idx = future_to_task[future]
                        task_info = tasks[task_idx]
                        results.append({
                            'success': False,
                            'item_key': task_info[0].get('item_key', ''),
                            'title': task_info[0].get('title', ''),
                            'worker_id': task_info[2],
                            'error': f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}',
                            'applied_collections': [],
                            'failed_collections': []
                        })
                        pbar.update(1)
        
        return results
    
    def save_application_results(self, results: List[Dict[str, Any]], 
                               original_df: pd.DataFrame) -> str:
        """ä¿å­˜åº”ç”¨ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"application_results_{timestamp}.xlsx"
        filepath = self.data_dir / filename
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åº”ç”¨ç»“æœåˆ° {filepath}...")
        
        # æ„å»ºç»“æœDataFrame
        result_data = []
        
        for result in results:
            item_key = result['item_key']
            
            # ä»åŸå§‹åˆ†ç±»ç»“æœä¸­è·å–å®Œæ•´ä¿¡æ¯
            original_row = original_df[original_df['item_key'] == item_key]
            if not original_row.empty:
                original_info = original_row.iloc[0].to_dict()
            else:
                original_info = {}
            
            # æ„å»ºç»“æœè¡Œ
            result_row = {
                'item_key': item_key,
                'title': result['title'],
                'item_type': original_info.get('item_type', ''),
                'authors': original_info.get('authors', ''),
                'recommended_collections': original_info.get('recommended_collections', ''),
                'recommended_count': original_info.get('recommended_count', 0),
                'application_success': result['success'],
                'applied_collections': '; '.join(result.get('applied_collections', [])),
                'applied_count': len(result.get('applied_collections', [])),
                'failed_collections': '; '.join(result.get('failed_collections', [])),
                'failed_count': len(result.get('failed_collections', [])),
                'error_message': result.get('error', ''),
                'worker_id': result.get('worker_id', ''),
                'analysis': original_info.get('analysis', '')
            }
            
            result_data.append(result_row)
        
        # ä¿å­˜åˆ°Excel
        result_df = pd.DataFrame(result_data)
        result_df.to_excel(filepath, index=False, engine='openpyxl')
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        successful_results = result_df[result_df['application_success'] == True]
        failed_results = result_df[result_df['application_success'] == False]
        
        print(f"\nğŸ“Š åº”ç”¨ç»“æœç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸåº”ç”¨: {len(successful_results)} ç¯‡")
        print(f"   âŒ åº”ç”¨å¤±è´¥: {len(failed_results)} ç¯‡")
        
        if len(successful_results) > 0:
            total_applied = successful_results['applied_count'].sum()
            print(f"   ğŸ“‚ æ€»å…±åº”ç”¨åˆ†ç±»: {total_applied} ä¸ª")
            print(f"   ğŸ“‚ å¹³å‡æ¯ç¯‡åˆ†ç±»æ•°: {successful_results['applied_count'].mean():.1f}")
            
            # ç»Ÿè®¡åº”ç”¨çš„åˆ†ç±»
            all_applied = []
            for applications in successful_results['applied_collections']:
                if applications:
                    all_applied.extend([cat.strip() for cat in applications.split(';')])
            
            if all_applied:
                from collections import Counter
                category_counts = Counter(all_applied)
                print(f"\nğŸ“‚ å·²åº”ç”¨åˆ†ç±»ç»Ÿè®¡:")
                for category, count in category_counts.most_common(10):
                    print(f"     - {category}: {count} ç¯‡")
        
        if len(failed_results) > 0:
            print(f"\nâŒ å¤±è´¥åŸå› ç»Ÿè®¡:")
            error_counts = failed_results['error_message'].value_counts()
            for error, count in error_counts.head(5).items():
                print(f"     - {error}: {count} ç¯‡")
        
        print(f"\nâœ… åº”ç”¨ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return str(filepath)
    
    def apply_and_save(self, limit: Optional[int] = None, start: int = 0) -> str:
        """æ‰§è¡Œåº”ç”¨å¹¶ä¿å­˜ç»“æœ"""
        print("ğŸš€ å¼€å§‹åˆ†ç±»åº”ç”¨ä»»åŠ¡...")
        
        # åŠ è½½åˆ†ç±»ç»“æœ
        results_df = self.load_latest_classification_results()
        if results_df is None:
            return ""
        
        # ç­›é€‰éœ€è¦åº”ç”¨çš„ç»“æœ
        filtered_df = self.filter_results_for_application(results_df)
        if len(filtered_df) == 0:
            print("âœ… æ²¡æœ‰éœ€è¦åº”ç”¨çš„åˆ†ç±»ç»“æœ")
            return ""
        
        # æ‰§è¡Œåº”ç”¨
        application_results = self.apply_classifications_batch(filtered_df, limit, start)
        
        # ä¿å­˜ç»“æœ
        result_file = self.save_application_results(application_results, results_df)
        
        return result_file


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”— Zoteroåˆ†ç±»åº”ç”¨å·¥å…· - 004")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ['ZOTERO_USER_ID', 'ZOTERO_API_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"\nâŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("\nè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š")
        print("export ZOTERO_USER_ID='ä½ çš„Zoteroç”¨æˆ·ID'")
        print("export ZOTERO_API_KEY='ä½ çš„Zotero APIå¯†é’¥'")
        return 1
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    limit = None
    start = 0
    max_workers = None
    rate_limit_delay = 0.1
    
    if len(sys.argv) > 1:
        try:
            limit = int(sys.argv[1])
        except ValueError:
            print("âŒ æ— æ•ˆçš„é™åˆ¶æ•°é‡å‚æ•°")
            return 1
    
    if len(sys.argv) > 2:
        try:
            start = int(sys.argv[2])
        except ValueError:
            print("âŒ æ— æ•ˆçš„èµ·å§‹ä½ç½®å‚æ•°")
            return 1
    
    if len(sys.argv) > 3:
        try:
            max_workers = int(sys.argv[3])
        except ValueError:
            print("âŒ æ— æ•ˆçš„è¿›ç¨‹æ•°å‚æ•°")
            return 1
    
    if len(sys.argv) > 4:
        try:
            rate_limit_delay = float(sys.argv[4])
        except ValueError:
            print("âŒ æ— æ•ˆçš„å»¶è¿Ÿå‚æ•°")
            return 1
    
    try:
        # åˆ›å»ºåº”ç”¨å™¨
        applier = ClassificationApplier(max_workers=max_workers, rate_limit_delay=rate_limit_delay)
        
        # è·å–é¢„è§ˆä¿¡æ¯
        print("\nğŸ” æ­£åœ¨åˆ†æåˆ†ç±»ç»“æœ...")
        
        # åŠ è½½åˆ†ç±»ç»“æœ
        results_df = applier.load_latest_classification_results()
        if results_df is None:
            print("âŒ æ— æ³•åŠ è½½åˆ†ç±»ç»“æœ")
            return 1
        
        # ç­›é€‰éœ€è¦åº”ç”¨çš„ç»“æœ
        filtered_df = applier.filter_results_for_application(results_df)
        if len(filtered_df) == 0:
            print("âœ… æ²¡æœ‰éœ€è¦åº”ç”¨çš„åˆ†ç±»ç»“æœ")
            return 0
        
        # è®¡ç®—å®é™…å¤„ç†èŒƒå›´
        total_count = len(filtered_df)
        if limit is None:
            actual_limit = total_count
            end_index = total_count
        else:
            actual_limit = min(limit, total_count - start)
            end_index = min(start + limit, total_count)
        
        if start >= total_count:
            print(f"âŒ èµ·å§‹ä½ç½® ({start+1}) è¶…å‡ºæ€»æ•°é‡ ({total_count})")
            return 1
        
        # è·å–å®é™…è¦å¤„ç†çš„æ•°æ®
        process_df = filtered_df.iloc[start:end_index]
        
        # ç»Ÿè®¡å³å°†åº”ç”¨çš„åˆ†ç±»ä¿¡æ¯
        total_classifications = 0
        classification_stats = {}
        
        for _, row in process_df.iterrows():
            recommended_collection_keys_str = row.get('recommended_collection_keys', '')
            if recommended_collection_keys_str and not pd.isna(recommended_collection_keys_str):
                recommended_collection_keys = [key.strip() for key in recommended_collection_keys_str.split(';') if key.strip()]
                total_classifications += len(recommended_collection_keys)
                
                for key in recommended_collection_keys:
                    classification_stats[key] = classification_stats.get(key, 0) + 1
        
        # æ˜¾ç¤ºé¢„è§ˆä¿¡æ¯
        print(f"\nğŸ“Š å³å°†æ‰§è¡Œçš„åˆ†ç±»åº”ç”¨é¢„è§ˆ:")
        print(f"   ğŸ“ æ€»åˆ†ç±»ç»“æœ: {len(results_df)} æ¡")
        print(f"   âœ… å¯åº”ç”¨ç»“æœ: {total_count} æ¡")
        print(f"   ğŸ¯ æœ¬æ¬¡å¤„ç†: {len(process_df)} ç¯‡æ–‡çŒ®")
        print(f"   ğŸ·ï¸ æ€»åˆ†ç±»æ“ä½œ: {total_classifications} ä¸ª")
        print(f"   ğŸ“ å¤„ç†èŒƒå›´: ç¬¬ {start+1} åˆ°ç¬¬ {end_index} æ¡")
        
        if classification_stats:
            print(f"\nğŸ”‘ å³å°†åº”ç”¨çš„åˆ†ç±»keysï¼ˆTop 10ï¼‰:")
            from collections import Counter
            sorted_stats = Counter(classification_stats).most_common(10)
            for collection_key, count in sorted_stats:
                print(f"     - {collection_key}: {count} ç¯‡")
        
        # ç¡®è®¤æ“ä½œ
        confirm = input(f"\nâš ï¸ ç¡®è®¤è¦å¼€å§‹åº”ç”¨åˆ†ç±»åˆ°Zoteroå—ï¼Ÿ(y/N): ").strip().lower()
        if confirm != 'y':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return 0
        
        # ç›´æ¥ä½¿ç”¨å·²ç­›é€‰çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼Œé¿å…é‡å¤åŠ è½½
        application_results = applier.apply_classifications_batch(process_df, None, 0)
        result_file = applier.save_application_results(application_results, results_df)
        
        if result_file:
            print(f"\nğŸ‰ åˆ†ç±»åº”ç”¨å®Œæˆï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result_file}")
            print(f"\nğŸ’¡ æ³¨æ„äº‹é¡¹:")
            print(f"   - è¯·æ£€æŸ¥Zoteroä¸­çš„åˆ†ç±»æ˜¯å¦æ­£ç¡®åº”ç”¨")
            print(f"   - å¦‚æœ‰é—®é¢˜å¯ä»¥æ ¹æ®ç»“æœæ–‡ä»¶è¿›è¡Œè°ƒæ•´")
        else:
            print("âŒ åº”ç”¨å¤±è´¥")
            return 1
        
        return 0
        
    except KeyboardInterrupt:
        print("\n\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main()) 